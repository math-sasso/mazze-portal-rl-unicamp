{
 "cells": [
  {
   "source": [
    "# $$ Monte Carlo - First Visit $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from libs\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import gym\n",
    "import gym_maze\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# imports from project\n",
    "from utils.plotting import plot_line_graphs_overlayed\n",
    "from utils.file_management import create_folder_structure\n",
    "from utils.file_management import save_nparray_to_folder"
   ]
  },
  {
   "source": [
    "## Setting Saving Directory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\msasso\\\\Desktop\\\\mazze-portal-rl-unicamp\\\\results\\\\MC_First_Visit'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "SAVING_RESULTS_FODLER = os.path.join(cwd,\"results\",\"MC_First_Visit\")\n",
    "SAVING_RESULTS_FODLER"
   ]
  },
  {
   "source": [
    "## Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"maze\" environment\n",
    "env = gym.make(\"maze-v0\")\n",
    "\n",
    "'''\n",
    "Defining the environment related constants\n",
    "'''\n",
    "# Number of discrete states (bucket) per state dimension\n",
    "MAZE_SIZE = tuple((env.observation_space.high + np.ones(env.observation_space.shape)).astype(int))\n",
    "NUM_BUCKETS = MAZE_SIZE  # one bucket per grid\n",
    "\n",
    "# Number of discrete actions\n",
    "NUM_ACTIONS = env.action_space.n  # [\"N\", \"S\", \"E\", \"W\"]\n",
    "# Bounds for each discrete state\n",
    "STATE_BOUNDS = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "\n",
    "'''\n",
    "Learning related constants\n",
    "'''\n",
    "MIN_EXPLORE_RATE = 0.001\n",
    "DECAY_FACTOR = np.prod(MAZE_SIZE, dtype=float) / 10.0\n",
    "\n",
    "'''\n",
    "Defining the simulation related constants\n",
    "'''\n",
    "NUM_EPISODES = 300\n",
    "MAX_T = np.prod(MAZE_SIZE, dtype=int) * 100\n",
    "STREAK_TO_END = 100\n",
    "SOLVED_T = np.prod(MAZE_SIZE, dtype=int)\n",
    "RENDER_MAZE = False\n",
    "ENABLE_RECORDING = False\n",
    "\n",
    "'''\n",
    "Q-Learning Related Constants\n",
    "'''\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "'''\n",
    "CREATING RESULTS SAVING FOLDERS\n",
    "'''\n",
    "SAVING_RESULTS_IMAGE_FODLER = os.path.join(SAVING_RESULTS_FODLER,\"images\")\n",
    "SAVING_RESULTS_VIDEO_FODLER = os.path.join(SAVING_RESULTS_FODLER,\"videos\")\n",
    "SAVING_RESULTS_ARRAY_FODLER = os.path.join(SAVING_RESULTS_FODLER,\"arrays\")\n",
    "create_folder_structure(SAVING_RESULTS_IMAGE_FODLER)\n",
    "create_folder_structure(SAVING_RESULTS_VIDEO_FODLER)\n",
    "create_folder_structure(SAVING_RESULTS_ARRAY_FODLER)\n",
    "\n",
    "'''\n",
    "ENABLE RECORDING\n",
    "'''\n",
    "if ENABLE_RECORDING:\n",
    "    env = gym.wrappers.Monitor(env,SAVING_RESULTS_VIDEO_FODLER, force=True)"
   ]
  },
  {
   "source": [
    "## Algorithm Simulation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(discount_factor:float= 0.99):\n",
    "    \"\"\"Simulate Q-Learning Algorythim .\n",
    "\n",
    "    Args:\n",
    "        discount_factor (float): gamma.\n",
    "\n",
    "    Returns:\n",
    "        np.array(reward_episode_list): Numpy array with the episodes rewards\n",
    "        np.array(explore_rate_episode_list): Numpy array with the episodes explore rates\n",
    "        np.array(num_streaks_episode_list): Numpy array with the episodes number of mazze completions\n",
    "        np.array(length_episode_list): Numpy array with the episodes lengths\n",
    "    \"\"\"\n",
    "\n",
    "    #creating episodes results lists\n",
    "    \n",
    "    reward_episode_list = []\n",
    "    explore_rate_episode_list = []\n",
    "    num_streaks_episode_list = []\n",
    "    length_episode_list = []\n",
    "\n",
    "    # Instantiating the related parameters\n",
    "    explore_rate = get_explore_rate(0)\n",
    "\n",
    "    # Render tha maze\n",
    "    env.render()\n",
    "\n",
    "    # Initialize q_table\n",
    "    q_table = np.zeros(NUM_BUCKETS + (NUM_ACTIONS,), dtype=float)\n",
    "\n",
    "    #Initializain num streaks\n",
    "    num_streaks= 0\n",
    "\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        # Episode Reinitialization\n",
    "        total_reward = 0\n",
    "        G_t = 0\n",
    "\n",
    "        # Sample k-th episode (sk,1, ak,1, rk,1, sk,2, . . . , sk,T ) given Ï€k\n",
    "        obv = env.reset()\n",
    "        state = state_to_bucket(obv)\n",
    "        episode_array = []\n",
    "        for t in range(MAX_T):\n",
    "            action = epsilon_greedy_police(state, explore_rate,q_table)\n",
    "            obv, reward, done, _ = env.step(action)\n",
    "            next_state = state_to_bucket(obv)\n",
    "            total_reward += reward\n",
    "            episode_array.append((state, action, reward)) # tuple transforma array([0,0]) em (0,0)\n",
    "\n",
    "            # Render tha maze\n",
    "            if RENDER_MAZE:\n",
    "                env.render()\n",
    "\n",
    "            if env.is_game_over():\n",
    "                sys.exit()\n",
    "\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {episode} finished after {t} time steps with total reward = {total_reward} (streak {num_streaks}).\")\n",
    "\n",
    "                if t <= SOLVED_T:\n",
    "                    num_streaks += 1\n",
    "                else:\n",
    "                    num_streaks = 0\n",
    "                break\n",
    "\n",
    "            elif t >= MAX_T - 1:\n",
    "                print(f\"Episode {episode} timed out at {t} with total reward = {total_reward}.\")\n",
    "\n",
    "            state = next_state\n",
    "        \n",
    "        # Find all states the we've visited in this episode\n",
    "        # We convert each state to a tuple so that we can use it as a dict key\n",
    "        sa_in_episode = set([(tuple(x[0]), x[1]) for x in episode_array])\n",
    "        for enum,(s, a) in enumerate(sa_in_episode):\n",
    "            N = enum +1\n",
    "            sa_pair = (s, a)\n",
    "            # Find the first occurance of the (state, action) pair in the episode\n",
    "            first_occurence_idx = next(i for i,x in enumerate(episode_array) if x[0] == s and x[1] == a)\n",
    "\n",
    "            # Sum up all rewards since the first occurance\n",
    "            G_t = sum([x[2]*(discount_factor**i) for i,x in enumerate(episode_array[first_occurence_idx:])])\n",
    "            # Calculate average return for this state over all sampled episodes\n",
    "\n",
    "            q_table[s + (a,)] +=  1/N * (G_t - q_table[(s) + (a,)])\n",
    "\n",
    "\n",
    "        # Appending Episode values to the list\n",
    "        reward_episode_list.append(total_reward)\n",
    "        explore_rate_episode_list.append(explore_rate)\n",
    "        num_streaks_episode_list.append(num_streaks)\n",
    "        length_episode_list.append(t)\n",
    "\n",
    "        # It's considered done when it's solved over 120 times consecutively\n",
    "        if num_streaks > STREAK_TO_END:\n",
    "            break\n",
    "\n",
    "        # Update parameters\n",
    "        explore_rate = get_explore_rate(episode)\n",
    "\n",
    "\n",
    "    print(\"FINAL ACTION TABLE \\n\")\n",
    "    acts = [\"N\", \"S\", \"E\", \"W\"]\n",
    "    for i in range(q_table.shape[2]):\n",
    "        print(f\"Action {acts[i]} table \\n {q_table[:,:,i]}\\n\")\n",
    "\n",
    "    return (np.array(reward_episode_list),\n",
    "            np.array(explore_rate_episode_list),\n",
    "            np.array(num_streaks_episode_list),\n",
    "            np.array(length_episode_list))\n"
   ]
  },
  {
   "source": [
    "## Epsilon Greed Exploration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_police(state, explore_rate,q_table):\n",
    "    # Select a random action\n",
    "    if random.random() < explore_rate:\n",
    "        action = env.action_space.sample()\n",
    "    # Select the action with the highest q\n",
    "    else:\n",
    "        action = int(np.argmax(q_table[state]))\n",
    "    return action"
   ]
  },
  {
   "source": [
    "## Updating EXPLORE_RATE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explore_rate(t):\n",
    "    return max(MIN_EXPLORE_RATE, min(0.8, 1.0 - math.log10((t+1)/DECAY_FACTOR)))"
   ]
  },
  {
   "source": [
    "## Function that returns the actual state"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_bucket(state):\n",
    "    bucket_indice = []\n",
    "    for i in range(len(state)):\n",
    "        if state[i] <= STATE_BOUNDS[i][0]:\n",
    "            bucket_index = 0\n",
    "        elif state[i] >= STATE_BOUNDS[i][1]:\n",
    "            bucket_index = NUM_BUCKETS[i] - 1\n",
    "        else:\n",
    "            # Mapping the state bounds to the bucket array\n",
    "            bound_width = STATE_BOUNDS[i][1] - STATE_BOUNDS[i][0]\n",
    "            offset = (NUM_BUCKETS[i]-1)*STATE_BOUNDS[i][0]/bound_width\n",
    "            scaling = (NUM_BUCKETS[i]-1)/bound_width\n",
    "            bucket_index = int(round(scaling*state[i] - offset))\n",
    "        bucket_indice.append(bucket_index)\n",
    "    return tuple(bucket_indice)"
   ]
  },
  {
   "source": [
    "## Running"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "error",
     "evalue": "display Surface quit",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-dfef4042ef31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mreward_episode_array\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mexplore_rate_episode_array\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_streaks_episode_array\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlength_episode_array\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m         \u001b[0msimulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscount_factor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDISCOUNT_FACTOR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mENABLE_RECORDING\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-41042bb5ccdd>\u001b[0m in \u001b[0;36msimulate\u001b[1;34m(discount_factor)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Render tha maze\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# Initialize q_table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msasso\\Desktop\\mazze-portal-rl-unicamp\\gym_maze\\envs\\maze_env.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaze_view\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquit_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaze_view\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msasso\\Desktop\\mazze-portal-rl-unicamp\\gym_maze\\envs\\maze_view_2d.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__game_over\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquit_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mimg_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msasso\\Desktop\\mazze-portal-rl-unicamp\\gym_maze\\envs\\maze_view_2d.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"human\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mimg_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__view_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__controller_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msasso\\Desktop\\mazze-portal-rl-unicamp\\gym_maze\\envs\\maze_view_2d.py\u001b[0m in \u001b[0;36m__view_update\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;31m# update the screen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackground\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaze_layer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "reward_episode_array,explore_rate_episode_array,num_streaks_episode_array,length_episode_array = \\\n",
    "        simulate(discount_factor = DISCOUNT_FACTOR)\n",
    "\n",
    "if ENABLE_RECORDING:\n",
    "    env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_saving_path = os.path.join(SAVING_RESULTS_IMAGE_FODLER,\"reward_episode\")\n",
    "plot_line_graphs_overlayed(x_list = [range(NUM_EPISODES)[:len(num_streaks_episode_array)]],\n",
    "                            y_list = [reward_episode_array],\n",
    "                            x_axe_name=\"Episode\", \n",
    "                            y_axe_name=\"Reward\",\n",
    "                            legends = [\"None_Marker\"],\n",
    "                            title = \"MC: Episode Reward by episode\" ,\n",
    "                            graph_saving_path = graph_saving_path)\n",
    "\n",
    "\n",
    "graph_saving_path = os.path.join(SAVING_RESULTS_IMAGE_FODLER,\"explore_rate_episode_array\")\n",
    "plot_line_graphs_overlayed(x_list = [range(NUM_EPISODES)[:len(num_streaks_episode_array)]],\n",
    "                            y_list = [explore_rate_episode_array],\n",
    "                            x_axe_name=\"Episode\", \n",
    "                            y_axe_name=\"Exploration Rate\",\n",
    "                            legends = [\"None_Marker\"],\n",
    "                            title = \"MC: Exploration Rate by episode\" ,\n",
    "                            graph_saving_path = graph_saving_path)\n",
    "\n",
    "\n",
    "\n",
    "graph_saving_path = os.path.join(SAVING_RESULTS_IMAGE_FODLER,\"num_streaks_episode_array\")\n",
    "plot_line_graphs_overlayed(x_list =[range(NUM_EPISODES)[:len(num_streaks_episode_array)]],\n",
    "                            y_list = [num_streaks_episode_array],\n",
    "                            x_axe_name=\"Episode\", \n",
    "                            y_axe_name=\"Number of streaks\",\n",
    "                            legends = [\"None_Marker\"],\n",
    "                            title = \"MC: Number of streaks by episode\" ,\n",
    "                            graph_saving_path = graph_saving_path)\n",
    "\n",
    "\n",
    "graph_saving_path = os.path.join(SAVING_RESULTS_IMAGE_FODLER,\"length_episode_array\")\n",
    "plot_line_graphs_overlayed(x_list = [range(NUM_EPISODES)[:len(num_streaks_episode_array)]],\n",
    "                            y_list = [length_episode_array],\n",
    "                            x_axe_name=\"Episode\", \n",
    "                            y_axe_name=\"Length of episode\",\n",
    "                            legends = [\"None_Marker\"],\n",
    "                            title = \"MC: Length of episode by episode\" ,\n",
    "                            graph_saving_path = graph_saving_path)"
   ]
  },
  {
   "source": [
    "## Saving numpy arrays from results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "reward_episode Shape:  (179,)\nexplore_rate Shape:  (179,)\nnum_streaks Shape:  (179,)\nlength_episode Shape:  (179,)\n"
     ]
    }
   ],
   "source": [
    "save_nparray_to_folder(reward_episode_array,SAVING_RESULTS_ARRAY_FODLER,\"reward_episode\")\n",
    "save_nparray_to_folder(explore_rate_episode_array,SAVING_RESULTS_ARRAY_FODLER,\"explore_rate\")\n",
    "save_nparray_to_folder(num_streaks_episode_array,SAVING_RESULTS_ARRAY_FODLER,\"num_streaks\")\n",
    "save_nparray_to_folder(length_episode_array,SAVING_RESULTS_ARRAY_FODLER,\"length_episode\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}