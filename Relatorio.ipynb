{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Relatorio.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ICVvXSNtvuj-",
        "xvS9dywKvuuh",
        "dnnI36SGxY3Y",
        "y_OsCby3wXIS",
        "D8km9jYazzMe",
        "dn6W9NgW8r-s",
        "mkB2tHnL9Go2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OboI4saPvcU9"
      },
      "source": [
        "# Reinforcement Learning: Project 1\r\n",
        "\r\n",
        "* Matheus Gustavo Alves Sasso - 158257\r\n",
        "* Fabiano\r\n",
        "* Lucas\r\n",
        "* Raphael"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICVvXSNtvuj-"
      },
      "source": [
        "## Goal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a32bJG3vuyU"
      },
      "source": [
        "### Problem Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldl7WENhxCED"
      },
      "source": [
        "R:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H3sfhOJvu2P"
      },
      "source": [
        "### The MDP formulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o-78v7exFlt"
      },
      "source": [
        "R:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdaNbcmawW66"
      },
      "source": [
        "### The discretization model adopted\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoLcp2BXxGLf"
      },
      "source": [
        "R:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ikar_GgwW-0"
      },
      "source": [
        "### How the problem was modeled \r\n",
        "**Evaluation according to: computational\r\n",
        "cost, optimality, influence of reward function, state and action space sizes**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPIivZvlxGx3"
      },
      "source": [
        "R:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC0J4haqwXEJ"
      },
      "source": [
        "### Implementation specifics and restrictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4qVItC1xHWL"
      },
      "source": [
        "R:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvS9dywKvuuh"
      },
      "source": [
        "## Problem Enverionment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ4GHoLexYkr"
      },
      "source": [
        "### The nature of your environment (episodic/not episodic, deterministic/stochastic)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-KGWxPIxvnx"
      },
      "source": [
        "R:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leYY-YTSxYp-"
      },
      "source": [
        "### What are your terminal states (when they exist)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxhVi8q8xvT5"
      },
      "source": [
        "R:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C03h4GXvxYtl"
      },
      "source": [
        "### How is your reward function defined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihxvKYozxu-Z"
      },
      "source": [
        "R:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx8cmju9xYyF"
      },
      "source": [
        "### All parameters employed in your methods (discount factor, step size, etc.)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvUQFm3exutp"
      },
      "source": [
        "R:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnnI36SGxY3Y"
      },
      "source": [
        "### Enverionment Class Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrF20a9nxuGK"
      },
      "source": [
        "## Code Here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_OsCby3wXIS"
      },
      "source": [
        "## Problem Implementations Without Linear Function Aproximators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9DfVfhLwXLz"
      },
      "source": [
        "### Monte Carlo Control\r\n",
        "* Initialize the value function to zero. \r\n",
        "* Use a time-varying scalar step-size of αt = 1/N(st, at)\r\n",
        "* Use an  Epsilon−greedy exploration strategy with Epsilon_t = N0/(N0+N(st)), where N0 is a constant, N(s) is the number of times that state s has been visited, and N(s, a) is the number of times that action a has been selected\r\n",
        "from state s. \r\n",
        "* You should define N0 to fit your problem. \r\n",
        "* Plot the optimal value function V(s) = maxaQ∗ (s, a).\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "393cixyl0WeY"
      },
      "source": [
        "Algorithim Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsmFc9Yl0e2A"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEZeUiD6zX1a"
      },
      "source": [
        "Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldynD963zD9Z"
      },
      "source": [
        "## Code Here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0cl8m_W8Ytm"
      },
      "source": [
        "Results (Graphs and tables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCTlIGTQ8Ytm"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUrHUajQ8Ytn"
      },
      "source": [
        "Relationship between adopted parameters x solution performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azYZRYX38Ytn"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hEQLlB88Ytn"
      },
      "source": [
        "Comparition With the traditional algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADkHp8fz8Ytn"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPdtT1Z3wXTk"
      },
      "source": [
        "### Q-learning (or some variation like DoubleQ-learning)\r\n",
        "* Initialize the action-value function to zero. \r\n",
        "* Use a random selection of actions whenever you have a draw among actions.\r\n",
        "* Use the same step-size and exploration schedules as MC Control. \r\n",
        "* Define the number of episodes and discount factor accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5bWxF9G0lGj"
      },
      "source": [
        "Algorithim Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeOvkG270nvd"
      },
      "source": [
        "R:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJpph5dRz2qC"
      },
      "source": [
        "Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQFtKqKEz4MP"
      },
      "source": [
        "## Code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOX7sdwc8XVu"
      },
      "source": [
        "Results (Graphs and tables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pZOjgIP8XVu"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP7BqkH78XVu"
      },
      "source": [
        "Relationship between adopted parameters x solution performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNyzudIA8XVv"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSIaaf4S8XVv"
      },
      "source": [
        "Comparition With the traditional algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnVWP1yF8XVv"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ_UJdr3wXPa"
      },
      "source": [
        "### SARSA(λ): \r\n",
        "* Initialize the action-value function to zero. \r\n",
        "* Use a random selection of actions whenever you have a draw among actions. \r\n",
        "* Use the same step-size and exploration schedules as MC Control.\r\n",
        "* Use the same number of episodes as Q-learning. \r\n",
        "* Vary λ ∈ {0, 0.2, ..., 1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcJ3p_KF0q4n"
      },
      "source": [
        "Algorithim Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eAKK1R10rEY"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ9O9imAzzIU"
      },
      "source": [
        "Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06H3Msr-0S0m"
      },
      "source": [
        "## Code Here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3jD__Qz8WCH"
      },
      "source": [
        "Results (Graphs and tables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKE1p_118WCH"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVQAuYc38WCH"
      },
      "source": [
        "Relationship between adopted parameters x solution performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjvp4A818WCI"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc-C6PZG8WCI"
      },
      "source": [
        "Comparition With the traditional algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1A2uhTE8WCI"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8km9jYazzMe"
      },
      "source": [
        "## Problem Implementations With Linear Function Aproximators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcXubztPzzQz"
      },
      "source": [
        "### Monte Carlo Control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDOuc8Y21Ffd"
      },
      "source": [
        "Algorithim Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0ngPIxV1Ffp"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2ad10451Ffp"
      },
      "source": [
        "Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4IyGuX21Ffq"
      },
      "source": [
        "## Code Here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9mU5qjc8UR2"
      },
      "source": [
        "Results (Graphs and tables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdL7HLJi8UR4"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLVY7uWP8UR4"
      },
      "source": [
        "Relationship between adopted parameters x solution performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL116Noo8UR4"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcNbAO868UR5"
      },
      "source": [
        "Comparition With the traditional algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XiF8CgS8UR5"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J3IrIjgzzVr"
      },
      "source": [
        "### Q-learning (or some variation like DoubleQ-learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2II_Ui8G1GCs"
      },
      "source": [
        "Algorithim Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-Y42JiT1GCt"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdTY6Doj1GCt"
      },
      "source": [
        "Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2jb9tJT1GCu"
      },
      "source": [
        "## Code Here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7iyqYFHzzen"
      },
      "source": [
        "Results (Graphs and tables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Llkt5ley1LZ_"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_lYLjde77_A"
      },
      "source": [
        "Relationship between adopted parameters x solution performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NpHlXDV78LC"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X6vVpnW8GHQ"
      },
      "source": [
        "Comparition With the traditional algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Msmj-HL8NVK"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAmJKGGQzzZ6"
      },
      "source": [
        "### SARSA(λ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aeD_m6-1Gtj"
      },
      "source": [
        "Algorithim Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1oo6Ef41Gtk"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAKTi8kD1Gtk"
      },
      "source": [
        "Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro3H7izd1Gtk"
      },
      "source": [
        "## Code Here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn6W9NgW8r-s"
      },
      "source": [
        "## Evaluation and Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EEQ4TwH8y0P"
      },
      "source": [
        "### The advantages and disadvantages of bootstrapping in your problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwIBMI2j9AZ7"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi98ntDw8y-o"
      },
      "source": [
        "### How the reward function influenced the quality of the solution. Was your group able to achieve the expected policy given the reward function defined?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66LIe5wk9A0a"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV7AVbgF8zCS"
      },
      "source": [
        "### How function approximation influenced the results. What were the advantages and disadvantages of using it in your problem?\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm63V0ZC9BVx"
      },
      "source": [
        "R: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkB2tHnL9Go2"
      },
      "source": [
        "## Member Cotributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00Sm0wzA9J5x"
      },
      "source": [
        "**Matheus**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7uX8fQY9J_5"
      },
      "source": [
        "**Fabiano**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5fvJa-9KDm"
      },
      "source": [
        "**Lucas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfDOlKeE9KRc"
      },
      "source": [
        "**Raphael**"
      ]
    }
  ]
}