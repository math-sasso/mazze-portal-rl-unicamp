{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OboI4saPvcU9"
   },
   "source": [
    "# Reinforcement Learning: Project 1\n",
    "\n",
    "* Matheus Gustavo Alves Sasso - 158257\n",
    "* Fabiano\n",
    "* Lucas\n",
    "* Rafael Cortez Sanchez - 094324"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICVvXSNtvuj-"
   },
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a32bJG3vuyU"
   },
   "source": [
    "### Definição do Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldl7WENhxCED"
   },
   "source": [
    "R: Dado um labirinto bidimensional e um agente capaz de navegar por esse labirinto, aprender o menor caminho entre dois pontos fornecidos. O labirinto também possui casas pareadas que transportam o agente de uma casa para a outra, adicionando complexidade ao problema. O código base utiliza a biblioteca do OpenAI Gym e está disponível [neste link](https://github.com/MattChanTK/gym-maze)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5H3sfhOJvu2P"
   },
   "source": [
    "### Formulação MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1o-78v7exFlt"
   },
   "source": [
    "R: O MDP é uma tupla $<S, P, A, R, \\gamma>$ tal que:\n",
    "\n",
    "$S = \\{s^{i, j} | i, j \\in \\mathbb{Z};  0 \\lt i \\leq m;  0 \\lt j \\leq n\\}$, em que $m$ e $n$ são as dimensões do labirinto em número inteiro de casas.\n",
    "\n",
    "$P = \\{P^{r, s} = \\begin{cases}\n",
    "                      1.0               & \\text{se não há paredes entre os estados r e s}\\\\\n",
    "                      0.0               & \\text{caso contrário}\n",
    "                  \\end{cases} \\}$\n",
    "\n",
    "$A = \\{\\text{cima}, \\text{baixo}, \\text{esquerda}, \\text{direita}\\}$\n",
    "\n",
    "$R = \\{R^s = \\begin{cases}\n",
    "                      1.0                        & \\text{caso s seja o estado final}\\\\\n",
    "                      \\frac{-0.1}{m \\times n}    & \\text{caso contrário}\n",
    "             \\end{cases} \\}$\n",
    "\n",
    "$\\gamma = 0.99$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdaNbcmawW66"
   },
   "source": [
    "### O modelo de discretização adotado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoLcp2BXxGLf"
   },
   "source": [
    "R: O problema em si já é discretizado, uma vez que o labirinto é composto por um número inteiro de casas dispostas em um plano. Cada estado é definido por duas coordenadas $i, j \\in \\mathbb{Z}$, representando uma casa desse plano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ikar_GgwW-0"
   },
   "source": [
    "### Como o problema foi modelado \n",
    "**Avaliar de acordo com: custo computacional, optimalidade, influência da função de recompensa, tamanhos dos espaços de estados e de ações**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPIivZvlxGx3"
   },
   "source": [
    "R: Considerando que as dimensões do mapa são $n$ e $m$, temos os seguintes tamanhos para os espaços de estados e ações:\n",
    "\n",
    "$|S| = n \\times m$\n",
    "\n",
    "$|A| = 4$ (As quatro direções cardinais no plano)\n",
    "\n",
    "O custo computacional de resolver a equação de Bellman por sistema linear de equações é, portanto:\n",
    "\n",
    "$O(n^3 m^3)$\n",
    "\n",
    "A solução ótima é uma política $\\pi$ que minimiza o número de ações para chegar à casa final do labirinto a partir da casa inicial. Há uma recompensa negativa quando o agente se move para casas que não são a casa final, e uma recompensa positiva quando o agente atinge a casa final. A recompensa negativa é inversamente proporcional ao tamanho do labirinto, de forma que a penalidade é maior quando se navega em um labirinto pequeno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC0J4haqwXEJ"
   },
   "source": [
    "### Especificidades e restrições da implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4qVItC1xHWL"
   },
   "source": [
    "R: O agente aprende a navegar em um único labirinto: a solução não é generalizada. A interface gráfica da solução foi implementada usando a biblioteca pygame e, caso o programa não seja devidamente finalizado, pode ser necessário reiniciar o interpretador Python antes de reinciar a execução do programa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvS9dywKvuuh"
   },
   "source": [
    "## Ambiente do Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZ4GHoLexYkr"
   },
   "source": [
    "### Natureza do seu ambiente (episódico/não episódico, determinístico/estocástico)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-KGWxPIxvnx"
   },
   "source": [
    "R: No ambiente utilizado, um episódio se inicia com o primeiro movimento do agente, e se encerra quando o agente atinge a casa final. Quanto aos algoritmos utilizados, alguns deles são episódicos e outros não.\n",
    "\n",
    "As ações do agente produzem resultados determinísticos. Se ele tentar se movimentar de uma casa A para uma adjacente casa B, ele o consegue com probabilidade de 100%, a menos que exista uma parede entre as duas casas. O agente não consegue permear essas paredes.\n",
    "\n",
    "Quando habilitadas, as casas de teletransporte também possuem comportamento determinístico: elas sempre transportarão o agente até a casa destino com 100% de certeza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leYY-YTSxYp-"
   },
   "source": [
    "### Quais são seus estados terminais (quando esses existirem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxhVi8q8xvT5"
   },
   "source": [
    "R: Compreende-se como estado terminal aquele em que o agente está posicionado na casa final. No labirinto há sempre apenas uma casa final, e portanto existe somente um estado terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C03h4GXvxYtl"
   },
   "source": [
    "### Como a função de recompensa é definida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihxvKYozxu-Z"
   },
   "source": [
    "R: Considerando $n$ e $m$ as dimensões do labirinto em casas, temos que:\n",
    "\n",
    "$R = 1.0$ para o estado final\n",
    "\n",
    "$R = - \\frac{0.1}{n \\times m}$ para qualquer estado não final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qx8cmju9xYyF"
   },
   "source": [
    "### Todos parâmetros empregados em seus métodos (desconto temporal, tamanho do passo, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvUQFm3exutp"
   },
   "source": [
    "R: Foram empregados os seguintes parâmetros:\n",
    "\n",
    "* **Desconto temporal $\\gamma$**: Esse hiper-parâmetro está relacionado com a importância dada aos estados anteriores em uma solução de aprendizado por reforço. Para um valor $\\gamma = 0$, o método é chamado de míope, e só considera a recompensa do estado mais recente.\n",
    "\n",
    "* **Taxa de aprendizado $\\alpha$**: Regula a velocidade com que o modelo tenta convergir para a solução ótima. Nos algoritmos que usam redes neurais profundas para Aproximadores de Função Linear (do inglês Linear Function Approximators, LFA), esse hiper-parâmetro define o quanto os parâmetros da rede são atualizados a cada iteração de treinamento.\n",
    "\n",
    "* **Taxa de exploração $\\epsilon$**: Em alguns algoritmos livres de modelo, o agente decide a cada passo se vai tomar uma ação de acordo com a política avaliada, ou se vai tomar uma ação aleatória para explorar outras possibilidades ($\\epsilon$-greedy). A probabilidade com que ele toma uma ação aleatória é chamada de taxa de exploração.\n",
    "\n",
    "* **Decaimento da taxa de exploração**: Os algoritmos que usam taxa de exploração geralmente definem um valor alto para esse hiper-parâmetro no início do processo, mas vão diminuindo a taxa conforme o modelo vai aprendendo. O decaimento da taxa de exploração dita a velocidade com que a taxa de exploração vai diminuir com a execução do algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnnI36SGxY3Y"
   },
   "source": [
    "### Código da classe ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View do ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeView2D:\n",
    "\n",
    "    def __init__(self, maze_name=\"Maze2D\", maze_file_path=None,\n",
    "                 maze_size=(30, 30), screen_size=(600, 600),\n",
    "                 has_loops=False, num_portals=0, enable_render=True):\n",
    "\n",
    "        # PyGame configurations\n",
    "        pygame.init()\n",
    "        pygame.display.set_caption(maze_name)\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.__game_over = False\n",
    "        self.__enable_render = enable_render\n",
    "\n",
    "        # Load a maze\n",
    "        if maze_file_path is None:\n",
    "            self.__maze = Maze(maze_size=maze_size, has_loops=has_loops, num_portals=num_portals)\n",
    "        else:\n",
    "            if not os.path.exists(maze_file_path):\n",
    "                dir_path = os.path.dirname(os.path.abspath(__file__))\n",
    "                rel_path = os.path.join(dir_path, \"maze_samples\", maze_file_path)\n",
    "                if os.path.exists(rel_path):\n",
    "                    maze_file_path = rel_path\n",
    "                else:\n",
    "                    raise FileExistsError(\"Cannot find %s.\" % maze_file_path)\n",
    "            self.__maze = Maze(maze_cells=Maze.load_maze(maze_file_path))\n",
    "\n",
    "        self.maze_size = self.__maze.maze_size\n",
    "        if self.__enable_render is True:\n",
    "            # to show the right and bottom border\n",
    "            self.screen = pygame.display.set_mode(screen_size)\n",
    "            self.__screen_size = tuple(map(sum, zip(screen_size, (-1, -1))))\n",
    "\n",
    "        # Set the starting point\n",
    "        self.__entrance = np.zeros(2, dtype=int)\n",
    "\n",
    "        # Set the Goal\n",
    "        self.__goal = np.array(self.maze_size) - np.array((1, 1))\n",
    "\n",
    "        # Create the Robot\n",
    "        self.__robot = self.entrance\n",
    "\n",
    "        if self.__enable_render is True:\n",
    "            # Create a background\n",
    "            self.background = pygame.Surface(self.screen.get_size()).convert()\n",
    "            self.background.fill((255, 255, 255))\n",
    "\n",
    "            # Create a layer for the maze\n",
    "            self.maze_layer = pygame.Surface(self.screen.get_size()).convert_alpha()\n",
    "            self.maze_layer.fill((0, 0, 0, 0,))\n",
    "\n",
    "            # show the maze\n",
    "            self.__draw_maze()\n",
    "\n",
    "            # show the portals\n",
    "            self.__draw_portals()\n",
    "\n",
    "            # show the robot\n",
    "            self.__draw_robot()\n",
    "\n",
    "            # show the entrance\n",
    "            self.__draw_entrance()\n",
    "\n",
    "            # show the goal\n",
    "            self.__draw_goal()\n",
    "\n",
    "    def update(self, mode=\"human\"):\n",
    "        try:\n",
    "            img_output = self.__view_update(mode)\n",
    "            self.__controller_update()\n",
    "        except Exception as e:\n",
    "            self.__game_over = True\n",
    "            self.quit_game()\n",
    "            raise e\n",
    "        else:\n",
    "            return img_output\n",
    "\n",
    "    def quit_game(self):\n",
    "        try:\n",
    "            self.__game_over = True\n",
    "            if self.__enable_render is True:\n",
    "                pygame.display.quit()\n",
    "            pygame.quit()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def move_robot(self, dir):\n",
    "        if dir not in self.__maze.COMPASS.keys():\n",
    "            raise ValueError(\"dir cannot be %s. The only valid dirs are %s.\"\n",
    "                             % (str(dir), str(self.__maze.COMPASS.keys())))\n",
    "\n",
    "        if self.__maze.is_open(self.__robot, dir):\n",
    "\n",
    "            # update the drawing\n",
    "            self.__draw_robot(transparency=0)\n",
    "\n",
    "            # move the robot\n",
    "            self.__robot += np.array(self.__maze.COMPASS[dir])\n",
    "            # if it's in a portal afterward\n",
    "            if self.maze.is_portal(self.robot):\n",
    "                self.__robot = np.array(self.maze.get_portal(tuple(self.robot)).teleport(tuple(self.robot)))\n",
    "            self.__draw_robot(transparency=255)\n",
    "\n",
    "    def reset_robot(self):\n",
    "\n",
    "        self.__draw_robot(transparency=0)\n",
    "        self.__robot = np.zeros(2, dtype=int)\n",
    "        self.__draw_robot(transparency=255)\n",
    "\n",
    "    def __controller_update(self):\n",
    "        if not self.__game_over:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    self.__game_over = True\n",
    "                    self.quit_game()\n",
    "\n",
    "    def __view_update(self, mode=\"human\"):\n",
    "        if not self.__game_over:\n",
    "            # update the robot's position\n",
    "            self.__draw_entrance()\n",
    "            self.__draw_goal()\n",
    "            self.__draw_portals()\n",
    "            self.__draw_robot()\n",
    "\n",
    "\n",
    "            # update the screen\n",
    "            self.screen.blit(self.background, (0, 0))\n",
    "            self.screen.blit(self.maze_layer,(0, 0))\n",
    "\n",
    "            if mode == \"human\":\n",
    "                pygame.display.flip()\n",
    "\n",
    "            return np.flipud(np.rot90(pygame.surfarray.array3d(pygame.display.get_surface())))\n",
    "\n",
    "    def __draw_maze(self):\n",
    "        \n",
    "        if self.__enable_render is False:\n",
    "            return\n",
    "        \n",
    "        line_colour = (0, 0, 0, 255)\n",
    "\n",
    "        # drawing the horizontal lines\n",
    "        for y in range(self.maze.MAZE_H + 1):\n",
    "            pygame.draw.line(self.maze_layer, line_colour, (0, y * self.CELL_H),\n",
    "                             (self.SCREEN_W, y * self.CELL_H))\n",
    "\n",
    "        # drawing the vertical lines\n",
    "        for x in range(self.maze.MAZE_W + 1):\n",
    "            pygame.draw.line(self.maze_layer, line_colour, (x * self.CELL_W, 0),\n",
    "                             (x * self.CELL_W, self.SCREEN_H))\n",
    "\n",
    "        # breaking the walls\n",
    "        for x in range(len(self.maze.maze_cells)):\n",
    "            for y in range (len(self.maze.maze_cells[x])):\n",
    "                # check the which walls are open in each cell\n",
    "                walls_status = self.maze.get_walls_status(self.maze.maze_cells[x, y])\n",
    "                dirs = \"\"\n",
    "                for dir, open in walls_status.items():\n",
    "                    if open:\n",
    "                        dirs += dir\n",
    "                self.__cover_walls(x, y, dirs)\n",
    "\n",
    "    def __cover_walls(self, x, y, dirs, colour=(0, 0, 255, 15)):\n",
    "\n",
    "        if self.__enable_render is False:\n",
    "            return\n",
    "        \n",
    "        dx = x * self.CELL_W\n",
    "        dy = y * self.CELL_H\n",
    "\n",
    "        if not isinstance(dirs, str):\n",
    "            raise TypeError(\"dirs must be a str.\")\n",
    "\n",
    "        for dir in dirs:\n",
    "            if dir == \"S\":\n",
    "                line_head = (dx + 1, dy + self.CELL_H)\n",
    "                line_tail = (dx + self.CELL_W - 1, dy + self.CELL_H)\n",
    "            elif dir == \"N\":\n",
    "                line_head = (dx + 1, dy)\n",
    "                line_tail = (dx + self.CELL_W - 1, dy)\n",
    "            elif dir == \"W\":\n",
    "                line_head = (dx, dy + 1)\n",
    "                line_tail = (dx, dy + self.CELL_H - 1)\n",
    "            elif dir == \"E\":\n",
    "                line_head = (dx + self.CELL_W, dy + 1)\n",
    "                line_tail = (dx + self.CELL_W, dy + self.CELL_H - 1)\n",
    "            else:\n",
    "                raise ValueError(\"The only valid directions are (N, S, E, W).\")\n",
    "\n",
    "            pygame.draw.line(self.maze_layer, colour, line_head, line_tail)\n",
    "\n",
    "    def __draw_robot(self, colour=(0, 0, 150), transparency=255):\n",
    "\n",
    "        if self.__enable_render is False:\n",
    "            return\n",
    "        \n",
    "        x = int(self.__robot[0] * self.CELL_W + self.CELL_W * 0.5 + 0.5)\n",
    "        y = int(self.__robot[1] * self.CELL_H + self.CELL_H * 0.5 + 0.5)\n",
    "        r = int(min(self.CELL_W, self.CELL_H)/5 + 0.5)\n",
    "\n",
    "        pygame.draw.circle(self.maze_layer, colour + (transparency,), (x, y), r)\n",
    "\n",
    "    def __draw_entrance(self, colour=(0, 0, 150), transparency=235):\n",
    "\n",
    "        self.__colour_cell(self.entrance, colour=colour, transparency=transparency)\n",
    "\n",
    "    def __draw_goal(self, colour=(150, 0, 0), transparency=235):\n",
    "\n",
    "        self.__colour_cell(self.goal, colour=colour, transparency=transparency)\n",
    "\n",
    "    def __draw_portals(self, transparency=160):\n",
    "\n",
    "        if self.__enable_render is False:\n",
    "            return\n",
    "        \n",
    "        colour_range = np.linspace(0, 255, len(self.maze.portals), dtype=int)\n",
    "        colour_i = 0\n",
    "        for portal in self.maze.portals:\n",
    "            colour = ((100 - colour_range[colour_i])% 255, colour_range[colour_i], 0)\n",
    "            colour_i += 1\n",
    "            for location in portal.locations:\n",
    "                self.__colour_cell(location, colour=colour, transparency=transparency)\n",
    "\n",
    "    def __colour_cell(self, cell, colour, transparency):\n",
    "\n",
    "        if self.__enable_render is False:\n",
    "            return\n",
    "\n",
    "        if not (isinstance(cell, (list, tuple, np.ndarray)) and len(cell) == 2):\n",
    "            raise TypeError(\"cell must a be a tuple, list, or numpy array of size 2\")\n",
    "\n",
    "        x = int(cell[0] * self.CELL_W + 0.5 + 1)\n",
    "        y = int(cell[1] * self.CELL_H + 0.5 + 1)\n",
    "        w = int(self.CELL_W + 0.5 - 1)\n",
    "        h = int(self.CELL_H + 0.5 - 1)\n",
    "        pygame.draw.rect(self.maze_layer, colour + (transparency,), (x, y, w, h))\n",
    "\n",
    "    @property\n",
    "    def maze(self):\n",
    "        return self.__maze\n",
    "\n",
    "    @property\n",
    "    def robot(self):\n",
    "        return self.__robot\n",
    "\n",
    "    @property\n",
    "    def entrance(self):\n",
    "        return self.__entrance\n",
    "\n",
    "    @property\n",
    "    def goal(self):\n",
    "        return self.__goal\n",
    "\n",
    "    @property\n",
    "    def game_over(self):\n",
    "        return self.__game_over\n",
    "\n",
    "    @property\n",
    "    def SCREEN_SIZE(self):\n",
    "        return tuple(self.__screen_size)\n",
    "\n",
    "    @property\n",
    "    def SCREEN_W(self):\n",
    "        return int(self.SCREEN_SIZE[0])\n",
    "\n",
    "    @property\n",
    "    def SCREEN_H(self):\n",
    "        return int(self.SCREEN_SIZE[1])\n",
    "\n",
    "    @property\n",
    "    def CELL_W(self):\n",
    "        return float(self.SCREEN_W) / float(self.maze.MAZE_W)\n",
    "\n",
    "    @property\n",
    "    def CELL_H(self):\n",
    "        return float(self.SCREEN_H) / float(self.maze.MAZE_H)\n",
    "\n",
    "\n",
    "class Maze:\n",
    "\n",
    "    COMPASS = {\n",
    "        \"N\": (0, -1),\n",
    "        \"E\": (1, 0),\n",
    "        \"S\": (0, 1),\n",
    "        \"W\": (-1, 0)\n",
    "    }\n",
    "\n",
    "    def __init__(self, maze_cells=None, maze_size=(10,10), has_loops=True, num_portals=0):\n",
    "\n",
    "        # maze member variables\n",
    "        self.maze_cells = maze_cells\n",
    "        self.has_loops = has_loops\n",
    "        self.__portals_dict = dict()\n",
    "        self.__portals = []\n",
    "        self.num_portals = num_portals\n",
    "\n",
    "        # Use existing one if exists\n",
    "        if self.maze_cells is not None:\n",
    "            if isinstance(self.maze_cells, (np.ndarray, np.generic)) and len(self.maze_cells.shape) == 2:\n",
    "                self.maze_size = tuple(maze_cells.shape)\n",
    "            else:\n",
    "                raise ValueError(\"maze_cells must be a 2D NumPy array.\")\n",
    "        # Otherwise, generate a random one\n",
    "        else:\n",
    "            # maze's configuration parameters\n",
    "            if not (isinstance(maze_size, (list, tuple)) and len(maze_size) == 2):\n",
    "                raise ValueError(\"maze_size must be a tuple: (width, height).\")\n",
    "            self.maze_size = maze_size\n",
    "\n",
    "            self._generate_maze()\n",
    "\n",
    "    def save_maze(self, file_path):\n",
    "\n",
    "        if not isinstance(file_path, str):\n",
    "            raise TypeError(\"Invalid file_path. It must be a str.\")\n",
    "\n",
    "        if not os.path.exists(os.path.dirname(file_path)):\n",
    "            raise ValueError(\"Cannot find the directory for %s.\" % file_path)\n",
    "\n",
    "        else:\n",
    "            np.save(file_path, self.maze_cells, allow_pickle=False, fix_imports=True)\n",
    "\n",
    "    @classmethod\n",
    "    def load_maze(cls, file_path):\n",
    "\n",
    "        if not isinstance(file_path, str):\n",
    "            raise TypeError(\"Invalid file_path. It must be a str.\")\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            raise ValueError(\"Cannot find %s.\" % file_path)\n",
    "\n",
    "        else:\n",
    "            return np.load(file_path, allow_pickle=False, fix_imports=True)\n",
    "\n",
    "    def _generate_maze(self):\n",
    "\n",
    "        # list of all cell locations\n",
    "        self.maze_cells = np.zeros(self.maze_size, dtype=int)\n",
    "\n",
    "        # Initializing constants and variables needed for maze generation\n",
    "        current_cell = (random.randint(0, self.MAZE_W-1), random.randint(0, self.MAZE_H-1))\n",
    "        num_cells_visited = 1\n",
    "        cell_stack = [current_cell]\n",
    "\n",
    "        # Continue until all cells are visited\n",
    "        while cell_stack:\n",
    "\n",
    "            # restart from a cell from the cell stack\n",
    "            current_cell = cell_stack.pop()\n",
    "            x0, y0 = current_cell\n",
    "\n",
    "            # find neighbours of the current cells that actually exist\n",
    "            neighbours = dict()\n",
    "            for dir_key, dir_val in self.COMPASS.items():\n",
    "                x1 = x0 + dir_val[0]\n",
    "                y1 = y0 + dir_val[1]\n",
    "                # if cell is within bounds\n",
    "                if 0 <= x1 < self.MAZE_W and 0 <= y1 < self.MAZE_H:\n",
    "                    # if all four walls still exist\n",
    "                    if self.all_walls_intact(self.maze_cells[x1, y1]):\n",
    "                    #if self.num_walls_broken(self.maze_cells[x1, y1]) <= 1:\n",
    "                        neighbours[dir_key] = (x1, y1)\n",
    "\n",
    "            # if there is a neighbour\n",
    "            if neighbours:\n",
    "                # select a random neighbour\n",
    "                dir = random.choice(tuple(neighbours.keys()))\n",
    "                x1, y1 = neighbours[dir]\n",
    "\n",
    "                # knock down the wall between the current cell and the selected neighbour\n",
    "                self.maze_cells[x1, y1] = self.__break_walls(self.maze_cells[x1, y1], self.__get_opposite_wall(dir))\n",
    "\n",
    "                # push the current cell location to the stack\n",
    "                cell_stack.append(current_cell)\n",
    "\n",
    "                # make the this neighbour cell the current cell\n",
    "                cell_stack.append((x1, y1))\n",
    "\n",
    "                # increment the visited cell count\n",
    "                num_cells_visited += 1\n",
    "\n",
    "        if self.has_loops:\n",
    "            self.__break_random_walls(0.2)\n",
    "\n",
    "        if self.num_portals > 0:\n",
    "            self.__set_random_portals(num_portal_sets=self.num_portals, set_size=2)\n",
    "\n",
    "    def __break_random_walls(self, percent):\n",
    "        # find some random cells to break\n",
    "        num_cells = int(round(self.MAZE_H*self.MAZE_W*percent))\n",
    "        cell_ids = random.sample(range(self.MAZE_W*self.MAZE_H), num_cells)\n",
    "\n",
    "        # for each of those walls\n",
    "        for cell_id in cell_ids:\n",
    "            x = cell_id % self.MAZE_H\n",
    "            y = int(cell_id/self.MAZE_H)\n",
    "\n",
    "            # randomize the compass order\n",
    "            dirs = random.sample(list(self.COMPASS.keys()), len(self.COMPASS))\n",
    "            for dir in dirs:\n",
    "                # break the wall if it's not already open\n",
    "                if self.is_breakable((x, y), dir):\n",
    "                    self.maze_cells[x, y] = self.__break_walls(self.maze_cells[x, y], dir)\n",
    "                    break\n",
    "\n",
    "    def __set_random_portals(self, num_portal_sets, set_size=2):\n",
    "        # find some random cells to break\n",
    "        num_portal_sets = int(num_portal_sets)\n",
    "        set_size = int(set_size)\n",
    "\n",
    "        # limit the maximum number of portal sets to the number of cells available.\n",
    "        max_portal_sets = int(self.MAZE_W * self.MAZE_H / set_size)\n",
    "        num_portal_sets = min(max_portal_sets, num_portal_sets)\n",
    "\n",
    "        # the first and last cells are reserved\n",
    "        cell_ids = random.sample(range(1, self.MAZE_W * self.MAZE_H - 1), num_portal_sets*set_size)\n",
    "\n",
    "        for i in range(num_portal_sets):\n",
    "            # sample the set_size number of sell\n",
    "            portal_cell_ids = random.sample(cell_ids, set_size)\n",
    "            portal_locations = []\n",
    "            for portal_cell_id in portal_cell_ids:\n",
    "                # remove the cell from the set of potential cell_ids\n",
    "                cell_ids.pop(cell_ids.index(portal_cell_id))\n",
    "                # convert portal ids to location\n",
    "                x = portal_cell_id % self.MAZE_H\n",
    "                y = int(portal_cell_id / self.MAZE_H)\n",
    "                portal_locations.append((x,y))\n",
    "            # append the new portal to the maze\n",
    "            portal = Portal(*portal_locations)\n",
    "            self.__portals.append(portal)\n",
    "\n",
    "            # create a dictionary of portals\n",
    "            for portal_location in portal_locations:\n",
    "                self.__portals_dict[portal_location] = portal\n",
    "\n",
    "    def is_open(self, cell_id, dir):\n",
    "        # check if it would be out-of-bound\n",
    "        x1 = cell_id[0] + self.COMPASS[dir][0]\n",
    "        y1 = cell_id[1] + self.COMPASS[dir][1]\n",
    "\n",
    "        # if cell is still within bounds after the move\n",
    "        if self.is_within_bound(x1, y1):\n",
    "            # check if the wall is opened\n",
    "            this_wall = bool(self.get_walls_status(self.maze_cells[cell_id[0], cell_id[1]])[dir])\n",
    "            other_wall = bool(self.get_walls_status(self.maze_cells[x1, y1])[self.__get_opposite_wall(dir)])\n",
    "            return this_wall or other_wall\n",
    "        return False\n",
    "\n",
    "    def is_breakable(self, cell_id, dir):\n",
    "        # check if it would be out-of-bound\n",
    "        x1 = cell_id[0] + self.COMPASS[dir][0]\n",
    "        y1 = cell_id[1] + self.COMPASS[dir][1]\n",
    "\n",
    "        return not self.is_open(cell_id, dir) and self.is_within_bound(x1, y1)\n",
    "\n",
    "    def is_within_bound(self, x, y):\n",
    "        # true if cell is still within bounds after the move\n",
    "        return 0 <= x < self.MAZE_W and 0 <= y < self.MAZE_H\n",
    "\n",
    "    def is_portal(self, cell):\n",
    "        return tuple(cell) in self.__portals_dict\n",
    "\n",
    "    @property\n",
    "    def portals(self):\n",
    "        return tuple(self.__portals)\n",
    "\n",
    "    def get_portal(self, cell):\n",
    "        if cell in self.__portals_dict:\n",
    "            return self.__portals_dict[cell]\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def MAZE_W(self):\n",
    "        return int(self.maze_size[0])\n",
    "\n",
    "    @property\n",
    "    def MAZE_H(self):\n",
    "        return int(self.maze_size[1])\n",
    "\n",
    "    @classmethod\n",
    "    def get_walls_status(cls, cell):\n",
    "        walls = {\n",
    "            \"N\" : (cell & 0x1) >> 0,\n",
    "            \"E\" : (cell & 0x2) >> 1,\n",
    "            \"S\" : (cell & 0x4) >> 2,\n",
    "            \"W\" : (cell & 0x8) >> 3,\n",
    "        }\n",
    "        return walls\n",
    "\n",
    "    @classmethod\n",
    "    def all_walls_intact(cls, cell):\n",
    "        return cell & 0xF == 0\n",
    "\n",
    "    @classmethod\n",
    "    def num_walls_broken(cls, cell):\n",
    "        walls = cls.get_walls_status(cell)\n",
    "        num_broken = 0\n",
    "        for wall_broken in walls.values():\n",
    "            num_broken += wall_broken\n",
    "        return num_broken\n",
    "\n",
    "    @classmethod\n",
    "    def __break_walls(cls, cell, dirs):\n",
    "        if \"N\" in dirs:\n",
    "            cell |= 0x1\n",
    "        if \"E\" in dirs:\n",
    "            cell |= 0x2\n",
    "        if \"S\" in dirs:\n",
    "            cell |= 0x4\n",
    "        if \"W\" in dirs:\n",
    "            cell |= 0x8\n",
    "        return cell\n",
    "\n",
    "    @classmethod\n",
    "    def __get_opposite_wall(cls, dirs):\n",
    "\n",
    "        if not isinstance(dirs, str):\n",
    "            raise TypeError(\"dirs must be a str.\")\n",
    "\n",
    "        opposite_dirs = \"\"\n",
    "\n",
    "        for dir in dirs:\n",
    "            if dir == \"N\":\n",
    "                opposite_dir = \"S\"\n",
    "            elif dir == \"S\":\n",
    "                opposite_dir = \"N\"\n",
    "            elif dir == \"E\":\n",
    "                opposite_dir = \"W\"\n",
    "            elif dir == \"W\":\n",
    "                opposite_dir = \"E\"\n",
    "            else:\n",
    "                raise ValueError(\"The only valid directions are (N, S, E, W).\")\n",
    "\n",
    "            opposite_dirs += opposite_dir\n",
    "\n",
    "        return opposite_dirs\n",
    "\n",
    "class Portal:\n",
    "\n",
    "    def __init__(self, *locations):\n",
    "\n",
    "        self.__locations = []\n",
    "        for location in locations:\n",
    "            if isinstance(location, (tuple, list)):\n",
    "                self.__locations.append(tuple(location))\n",
    "            else:\n",
    "                raise ValueError(\"location must be a list or a tuple.\")\n",
    "\n",
    "    def teleport(self, cell):\n",
    "        if cell in self.locations:\n",
    "            return self.locations[(self.locations.index(cell) + 1) % len(self.locations)]\n",
    "        return cell\n",
    "\n",
    "    def get_index(self, cell):\n",
    "        return self.locations.index(cell)\n",
    "\n",
    "    @property\n",
    "    def locations(self):\n",
    "        return self.__locations\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     maze = MazeView2D(screen_size= (500, 500), maze_size=(10,10))\n",
    "#     maze.update()\n",
    "#     input(\"Enter any key to quit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classe do ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrF20a9nxuGK"
   },
   "outputs": [],
   "source": [
    "class MazeEnv(gym.Env):\n",
    "    metadata = {\n",
    "        \"render.modes\": [\"human\", \"rgb_array\"],\n",
    "    }\n",
    "\n",
    "    ACTION = [\"N\", \"S\", \"E\", \"W\"]\n",
    "\n",
    "    def __init__(self, maze_file=None, maze_size=None, mode=None, enable_render=True):\n",
    "\n",
    "        self.viewer = None\n",
    "        self.enable_render = enable_render\n",
    "\n",
    "        if maze_file:\n",
    "            self.maze_view = MazeView2D(maze_name=\"OpenAI Gym - Maze (%s)\" % maze_file,\n",
    "                                        maze_file_path=maze_file,\n",
    "                                        screen_size=(640, 640), \n",
    "                                        enable_render=enable_render)\n",
    "        elif maze_size:\n",
    "            if mode == \"plus\":\n",
    "                has_loops = True\n",
    "                num_portals = int(round(min(maze_size)/3))\n",
    "            else:\n",
    "                has_loops = False\n",
    "                num_portals = 0\n",
    "\n",
    "            self.maze_view = MazeView2D(maze_name=\"OpenAI Gym - Maze (%d x %d)\" % maze_size,\n",
    "                                        maze_size=maze_size, screen_size=(640, 640),\n",
    "                                        has_loops=has_loops, num_portals=num_portals,\n",
    "                                        enable_render=enable_render)\n",
    "        else:\n",
    "            raise AttributeError(\"One must supply either a maze_file path (str) or the maze_size (tuple of length 2)\")\n",
    "\n",
    "        self.maze_size = self.maze_view.maze_size\n",
    "\n",
    "        # forward or backward in each dimension\n",
    "        self.action_space = spaces.Discrete(2*len(self.maze_size))\n",
    "\n",
    "        # observation is the x, y coordinate of the grid\n",
    "        low = np.zeros(len(self.maze_size), dtype=int)\n",
    "        high =  np.array(self.maze_size, dtype=int) - np.ones(len(self.maze_size), dtype=int)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.int64)\n",
    "\n",
    "        # initial condition\n",
    "        self.state = None\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "        # Simulation related variables.\n",
    "        self.seed()\n",
    "        self.reset()\n",
    "\n",
    "        # Just need to initialize the relevant attributes\n",
    "        self.configure()\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.enable_render is True:\n",
    "            self.maze_view.quit_game()\n",
    "\n",
    "    def configure(self, display=None):\n",
    "        self.display = display\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        if isinstance(action, int):\n",
    "            self.maze_view.move_robot(self.ACTION[action])\n",
    "        else:\n",
    "            self.maze_view.move_robot(action)\n",
    "\n",
    "        if np.array_equal(self.maze_view.robot, self.maze_view.goal):\n",
    "            reward = 1\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -0.1/(self.maze_size[0]*self.maze_size[1])\n",
    "            done = False\n",
    "\n",
    "        self.state = self.maze_view.robot\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.maze_view.reset_robot()\n",
    "        self.state = np.zeros(2)\n",
    "        self.steps_beyond_done = None\n",
    "        self.done = False\n",
    "        return self.state\n",
    "\n",
    "    def is_game_over(self):\n",
    "        return self.maze_view.game_over\n",
    "\n",
    "    def render(self, mode=\"human\", close=False):\n",
    "        if close:\n",
    "            self.maze_view.quit_game()\n",
    "\n",
    "        return self.maze_view.update(mode)\n",
    "\n",
    "\n",
    "class MazeEnvSample5x5(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True):\n",
    "        super(MazeEnvSample5x5, self).__init__(maze_file=\"maze2d_5x5.npy\", enable_render=enable_render)\n",
    "\n",
    "\n",
    "class MazeEnvRandom5x5(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True):\n",
    "        super(MazeEnvRandom5x5, self).__init__(maze_size=(5, 5), enable_render=enable_render)\n",
    "\n",
    "\n",
    "class MazeEnvSample10x10(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True):\n",
    "        super(MazeEnvSample10x10, self).__init__(maze_file=\"maze2d_10x10.npy\", enable_render=enable_render)\n",
    "\n",
    "\n",
    "class MazeEnvRandom10x10(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True):\n",
    "        super(MazeEnvRandom10x10, self).__init__(maze_size=(10, 10), enable_render=enable_render)\n",
    "\n",
    "\n",
    "class MazeEnvSample3x3(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True):\n",
    "        super(MazeEnvSample3x3, self).__init__(maze_file=\"maze2d_3x3.npy\", enable_render=enable_render)\n",
    "\n",
    "\n",
    "class MazeEnvRandom3x3(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True):\n",
    "        super(MazeEnvRandom3x3, self).__init__(maze_size=(3, 3), enable_render=enable_render)\n",
    "\n",
    "\n",
    "class MazeEnvSample100x100(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True):\n",
    "        super(MazeEnvSample100x100, self).__init__(maze_file=\"maze2d_100x100.npy\", enable_render=enable_render)\n",
    "\n",
    "\n",
    "class MazeEnvRandom100x100(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True):\n",
    "        super(MazeEnvRandom100x100, self).__init__(maze_size=(100, 100), enable_render=enable_render)\n",
    "\n",
    "\n",
    "class MazeEnvRandom10x10Plus(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True):\n",
    "        super(MazeEnvRandom10x10Plus, self).__init__(maze_size=(10, 10), mode=\"plus\", enable_render=enable_render)\n",
    "\n",
    "\n",
    "class MazeEnvRandom20x20Plus(MazeEnv):\n",
    "\n",
    "    def __init__(self, enable_render=True):\n",
    "        super(MazeEnvRandom20x20Plus, self).__init__(maze_size=(20, 20), mode=\"plus\", enable_render=enable_render)\n",
    "\n",
    "\n",
    "class MazeEnvRandom30x30Plus(MazeEnv):\n",
    "    def __init__(self, enable_render=True):\n",
    "        super(MazeEnvRandom30x30Plus, self).__init__(maze_size=(30, 30), mode=\"plus\", enable_render=enable_render)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_OsCby3wXIS"
   },
   "source": [
    "## Implementações sem Linear Function Aproximators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9DfVfhLwXLz"
   },
   "source": [
    "### Monte Carlo Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "393cixyl0WeY"
   },
   "source": [
    "#### Descrição do algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsmFc9Yl0e2A"
   },
   "source": [
    "R: O método de Monte Carlo atualiza os valores da tabela Q somente ao fim de cada episódio. Assim sendo, os episódios iniciais podem demorar a encontrar a solução, dada a natureza aleatória das primeiras ações tomadas. Desta forma, somente com a conclusão dos primeiros episódios, a tabela Q passa ser atualizada de acordo com os reforços (que penalizam as ações que não finalizam a saída do labirinto e premiam o atingimento da solução do labirinto) e o algoritmo passa a convergir.\n",
    "A estrutura da simulação segue, portanto, a estrutura descrita acima, em que existe um loop que varre os episódios até que a convergência seja obtida, e um loop interno que, uma vez finalizado o episódio, faz a atualização da tabela Q.\n",
    "Para o caso do algoritmo de Monte Carlo utilizado, apenas uma visita de cada estado é considerada por episódio. Para que isto seja possível, é necessário armazenar o histórico de visitas do agente durante toda a simulação, de maneira que estados repetidos sejam desconsiderados no processo de atualização de Q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEZeUiD6zX1a"
   },
   "source": [
    "#### Implementação e resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(discount_factor:float= 0.99):\n",
    "    \"\"\"Simulate Q-Learning Algorythim .\n",
    "\n",
    "    Args:\n",
    "        discount_factor (float): gamma.\n",
    "\n",
    "    Returns:\n",
    "        np.array(reward_episode_list): Numpy array with the episodes rewards\n",
    "        np.array(explore_rate_episode_list): Numpy array with the episodes explore rates\n",
    "        np.array(num_streaks_episode_list): Numpy array with the episodes number of mazze completions\n",
    "        np.array(length_episode_list): Numpy array with the episodes lengths\n",
    "    \"\"\"\n",
    "\n",
    "    #creating episodes results lists\n",
    "    \n",
    "    reward_episode_list = []\n",
    "    explore_rate_episode_list = []\n",
    "    num_streaks_episode_list = []\n",
    "    length_episode_list = []\n",
    "\n",
    "    # Instantiating the related parameters\n",
    "    explore_rate = get_explore_rate(0)\n",
    "\n",
    "    # Render tha maze\n",
    "    env.render()\n",
    "\n",
    "    # Initialize q_table\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    #Initializain num streaks\n",
    "    num_streaks= 0\n",
    "\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        # Episode Reinitialization\n",
    "        total_reward = 0\n",
    "        G_t = 0\n",
    "\n",
    "        # Sample k-th episode (sk,1, ak,1, rk,1, sk,2, . . . , sk,T ) given πk\n",
    "        obv = env.reset()\n",
    "        state = state_to_bucket(obv)\n",
    "        episode_array = []\n",
    "        for t in range(MAX_T):\n",
    "            action = epsilon_greedy_police(state, explore_rate,Q)\n",
    "            obv, reward, done, _ = env.step(action)\n",
    "            next_state = state_to_bucket(obv)\n",
    "            total_reward += reward\n",
    "            episode_array.append((state, action, reward)) # tuple transforma array([0,0]) em (0,0)\n",
    "\n",
    "            # Render tha maze\n",
    "            if RENDER_MAZE:\n",
    "                env.render()\n",
    "\n",
    "            if env.is_game_over():\n",
    "                sys.exit()\n",
    "\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {episode} finished after {t} time steps with total reward = {total_reward} (streak {num_streaks}).\")\n",
    "\n",
    "                if t <= SOLVED_T:\n",
    "                    num_streaks += 1\n",
    "                else:\n",
    "                    num_streaks = 0\n",
    "                break\n",
    "\n",
    "            elif t >= MAX_T - 1:\n",
    "                print(f\"Episode {episode} timed out at {t} with total reward = {total_reward}.\")\n",
    "\n",
    "            state = next_state\n",
    "        \n",
    "        # Find all states the we've visited in this episode\n",
    "        # We convert each state to a tuple so that we can use it as a dict key\n",
    "        sa_in_episode = set([(tuple(x[0]), x[1]) for x in episode_array])\n",
    "        N = len(sa_in_episode)\n",
    "        for s, a in sa_in_episode:\n",
    "            sa_pair = (s, a)\n",
    "            # Find the first occurance of the (state, action) pair in the episode\n",
    "            first_occurence_idx = next(i for i,x in enumerate(episode_array) if x[0] == s and x[1] == a)\n",
    "\n",
    "            # Sum up all rewards since the first occurance\n",
    "            G_t = sum([x[2]*(discount_factor**i) for i,x in enumerate(episode_array[first_occurence_idx:])])\n",
    "            \n",
    "            # Calculate average return for this state over all sampled episodes\n",
    "            Q[s][a] +=  1/N * (G_t - Q[s][a])\n",
    "\n",
    "\n",
    "        # Appending Episode values to the list\n",
    "        reward_episode_list.append(total_reward)\n",
    "        explore_rate_episode_list.append(explore_rate)\n",
    "        num_streaks_episode_list.append(num_streaks)\n",
    "        length_episode_list.append(t)\n",
    "\n",
    "        # It's considered done when it's solved over 120 times consecutively\n",
    "        if num_streaks > STREAK_TO_END:\n",
    "            break\n",
    "\n",
    "        # Update parameters\n",
    "        explore_rate = get_explore_rate(episode)\n",
    "\n",
    "\n",
    "    return (Q,np.array(reward_episode_list),\n",
    "            np.array(explore_rate_episode_list),\n",
    "            np.array(num_streaks_episode_list),\n",
    "            np.array(length_episode_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azYZRYX38Ytn"
   },
   "source": [
    "Recompença pelo episodio \n",
    "\n",
    "![MC1](results\\MC_First_Visit\\images\\reward_episode.png)\n",
    "\n",
    "Tamanho do episódio por episódio\n",
    "\n",
    "![MC2](results\\MC_First_Visit\\images\\length_episode.png)\n",
    "\n",
    "Número de acertos por episódio\n",
    "\n",
    "![MC3](results\\MC_First_Visit\\images\\num_streaks_episode.png)\n",
    "\n",
    "Taxa de exploração por episódio\n",
    "\n",
    "![MC4](results\\MC_First_Visit\\images\\explore_rate_episode.png)\n",
    "\n",
    "Mapa de direções\n",
    "\n",
    "![MC4](results\\MC_First_Visit\\images\\directions_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPdtT1Z3wXTk"
   },
   "source": [
    "### Q-learning (or some variation like DoubleQ-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5bWxF9G0lGj"
   },
   "source": [
    "#### Algorithim Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeOvkG270nvd"
   },
   "source": [
    "R: O algorítimo Q Learning um algorítimo off policy, pois a função utilizada para aprendizado é diferente daquela em que o mesmo executa as ações. O Q Learning utiliza a política epsilon greedy para atualizar a tabela Q e a política greedy para tomada de ações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJpph5dRz2qC"
   },
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQFtKqKEz4MP"
   },
   "outputs": [],
   "source": [
    "def simulate(discount_factor:float= 0.99,\n",
    "             learning_rate:float= 0.01):\n",
    "    \"\"\"Simulate Q-Learning Algorythim .\n",
    "\n",
    "    Args:\n",
    "        discount_factor (float): gamma.\n",
    "\n",
    "    Returns:\n",
    "        np.array(reward_episode_list): Numpy array with the episodes rewards\n",
    "        np.array(explore_rate_episode_list): Numpy array with the episodes explore rates\n",
    "        np.array(num_streaks_episode_list): Numpy array with the episodes number of mazze completions\n",
    "        np.array(length_episode_list): Numpy array with the episodes lengths\n",
    "    \"\"\"\n",
    "\n",
    "    #creating episodes results lists\n",
    "    \n",
    "    reward_episode_list = []\n",
    "    explore_rate_episode_list = []\n",
    "    num_streaks_episode_list = []\n",
    "    length_episode_list = []\n",
    "\n",
    "    # Instantiating the learning related parameters\n",
    "    explore_rate = get_explore_rate(0)\n",
    "\n",
    "    # Render tha maze\n",
    "    env.render()\n",
    "\n",
    "    # Initialize q_table\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    #Initializain num streaks\n",
    "    num_streaks= 0\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "\n",
    "        # Reset the environment\n",
    "        obv = env.reset()\n",
    "\n",
    "        # the initial state\n",
    "        state = state_to_bucket(obv)\n",
    "        total_reward = 0\n",
    "\n",
    "\n",
    "        for t in range(MAX_T):\n",
    "\n",
    "            # Select an action\n",
    "            action = epsilon_greedy_police(state, explore_rate,Q)\n",
    "\n",
    "            # execute the action\n",
    "            obv, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Observe the result\n",
    "            next_state = state_to_bucket(obv)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Update the Q based on the result\n",
    "            best_q = np.amax(Q[next_state])\n",
    "            Q[state][action] += learning_rate * (reward + discount_factor * (best_q) - Q[state][action])\n",
    "\n",
    "            # Setting up for the next iteration\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "            # Render tha maze\n",
    "            if RENDER_MAZE:\n",
    "                env.render()\n",
    "\n",
    "            if env.is_game_over():\n",
    "                sys.exit()\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {episode} finished after {t} time steps with total reward = {total_reward} (streak {num_streaks}).\")\n",
    "\n",
    "                if t <= SOLVED_T:\n",
    "                    num_streaks += 1\n",
    "                else:\n",
    "                    num_streaks = 0\n",
    "                break\n",
    "\n",
    "            elif t >= MAX_T - 1:\n",
    "                print(f\"Episode {episode} timed out at {t} with total reward = {total_reward}.\")\n",
    "\n",
    "        # Appending Episode values to the list\n",
    "        reward_episode_list.append(total_reward)\n",
    "        explore_rate_episode_list.append(explore_rate)\n",
    "        num_streaks_episode_list.append(num_streaks)\n",
    "        length_episode_list.append(t)\n",
    "\n",
    "        # It's considered done when it's solved over 120 times consecutively\n",
    "        if num_streaks > STREAK_TO_END:\n",
    "            break\n",
    "\n",
    "        # Update parameters\n",
    "        explore_rate = get_explore_rate(episode)\n",
    "\n",
    "\n",
    "    return (Q,np.array(reward_episode_list),\n",
    "            np.array(explore_rate_episode_list),\n",
    "            np.array(num_streaks_episode_list),\n",
    "            np.array(length_episode_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOX7sdwc8XVu"
   },
   "source": [
    "#### Results (Graphs and tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pZOjgIP8XVu"
   },
   "source": [
    "Recompença pelo episodio \n",
    "\n",
    "![Q1](results/Q_LEANING/ripped_images/1.png)\n",
    "\n",
    "Tamanho do episódio por episódio\n",
    "\n",
    "![Q2](results/Q_LEANING/ripped_images/4.png)\n",
    "\n",
    "Número de acertos por episódio\n",
    "\n",
    "![Q3](results/Q_LEANING/ripped_images/3.png)\n",
    "\n",
    "Taxa de exploração por episódio\n",
    "\n",
    "![Q4](results/Q_LEANING/ripped_images/2.png)\n",
    "\n",
    "Mapa de direções\n",
    "\n",
    "![Q5](results\\Q_LEARNING\\images\\directions_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vP7BqkH78XVu"
   },
   "source": [
    "Relationship between adopted parameters x solution performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNyzudIA8XVv"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSIaaf4S8XVv"
   },
   "source": [
    "Comparition With the traditional algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnVWP1yF8XVv"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJ_UJdr3wXPa"
   },
   "source": [
    "### SARSA(λ): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcJ3p_KF0q4n"
   },
   "source": [
    "#### Algorithim Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eAKK1R10rEY"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ9O9imAzzIU"
   },
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06H3Msr-0S0m"
   },
   "outputs": [],
   "source": [
    "def simulate(discount_factor:float=0.99,\n",
    "            learning_rate:float=0.01,\n",
    "            epsilon:float= 1,\n",
    "            epsilon_decay:float= 0.999,\n",
    "            trace_decay:int= 0):\n",
    "\n",
    "\n",
    "    \"\"\"Simulate Sarsa Algorythim .\n",
    "\n",
    "    Args:\n",
    "        discount_factor (float): gamma.\n",
    "        epsilon (float): gamma.\n",
    "        epsilon_decay (float): gamma.\n",
    "        trace_decay (float): gamma.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        np.array(reward_episode_list): Numpy array with the episodes rewards\n",
    "        np.array(explore_rate_episode_list): Numpy array with the episodes explore rates\n",
    "        np.array(learning_rate_episode_list): Numpy array with the episodes learning rates\n",
    "        np.array(num_streaks_episode_list): Numpy array with the episodes number of mazze completions\n",
    "        np.array(length_episode_list): Numpy array with the episodes lengths\n",
    "    \"\"\"\n",
    "    \n",
    "    #creating episodes results lists\n",
    "    \n",
    "    reward_episode_list = []\n",
    "    explore_rate_episode_list = []\n",
    "    num_streaks_episode_list = []\n",
    "    length_episode_list = []\n",
    "\n",
    "\n",
    "    # Instantiating the learning related parameters\n",
    "    #learning_rate = get_learning_rate(0) # alpha\n",
    "    explore_rate = get_explore_rate(0)\n",
    "     \n",
    "    # Render tha maze\n",
    "    env.render()\n",
    "    \n",
    "    #q_table # inicializar ela aqui.\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "\n",
    "    #Initializain num streaks\n",
    "    num_streaks= 0\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "\n",
    " \n",
    "        # Reset the environment\n",
    "        obv = env.reset()\n",
    "        \n",
    "        # initialize Reward\n",
    "        total_reward = 0\n",
    "        \n",
    "        # initialize episolon decay\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "        # E(S,A) = 0\n",
    "        E = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "        \n",
    "        # Initialize state\n",
    "        state = state_to_bucket(obv)\n",
    "        \n",
    "        # initialize action    # VERIFICAR SE SEGUE A POLITICA   \n",
    "        action = epsilon_greedy_police(state, explore_rate, Q)\n",
    "        \n",
    "        for t in range(MAX_T):\n",
    "\n",
    "            # Select an action\n",
    "            next_action = epsilon_greedy_police(state, explore_rate, Q)\n",
    "            \n",
    "            # execute the action\n",
    "            obv, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Observe the result\n",
    "            next_state = state_to_bucket(obv)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # current_q\n",
    "            current_q = Q[state][action] \n",
    "            \n",
    "            # next_q\n",
    "            next_q = Q[next_state][next_action]\n",
    "            \n",
    "            # TD = r + gamma * Q(S', A') - Q(S,A)\n",
    "            # TD = sigma\n",
    "            TD = reward + (discount_factor * (next_q) - current_q )\n",
    "            \n",
    "            # E(S,A) - E(S,A) + 1\n",
    "            E[state][action] += 1\n",
    "            \n",
    "            #For all s E S, a E A(s)\n",
    "            for s, _ in Q.items():\n",
    "                Q[s][:] += learning_rate * TD * E[s][:]\n",
    "                E[s][:] *= trace_decay * discount_factor\n",
    "\n",
    "\n",
    "            #S - S'\n",
    "            state = next_state\n",
    "            \n",
    "            # A - A'\n",
    "            action = next_action\n",
    "            \n",
    "\n",
    "            # Render tha maze\n",
    "            if RENDER_MAZE:\n",
    "                env.render()\n",
    "\n",
    "            if env.is_game_over():\n",
    "                sys.exit()\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {episode} finished after {t} time steps with total reward = {total_reward} (streak {num_streaks}).\")\n",
    "\n",
    "                if t <= SOLVED_T:\n",
    "                    num_streaks += 1\n",
    "                else:\n",
    "                    num_streaks = 0\n",
    "                break\n",
    "\n",
    "            elif t >= MAX_T - 1:\n",
    "                print(f\"Episode {episode} timed out at {t} with total reward = {total_reward}.\")\n",
    "\n",
    "        \n",
    "\n",
    "        # Appending Episode values to the list\n",
    "        reward_episode_list.append(total_reward)\n",
    "        explore_rate_episode_list.append(explore_rate)\n",
    "        num_streaks_episode_list.append(num_streaks)\n",
    "        length_episode_list.append(t)\n",
    "\n",
    "        # It's considered done when it's solved over 120 times consecutively\n",
    "        if num_streaks > STREAK_TO_END:\n",
    "            break\n",
    "\n",
    "        # Update parameters\n",
    "        explore_rate = get_explore_rate(episode)\n",
    "\n",
    "\n",
    "    return (Q,np.array(reward_episode_list),\n",
    "            np.array(explore_rate_episode_list),\n",
    "            np.array(num_streaks_episode_list),\n",
    "            np.array(length_episode_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3jD__Qz8WCH"
   },
   "source": [
    "Results (Graphs and tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKE1p_118WCH"
   },
   "source": [
    "Recompença pelo episodio \n",
    "\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/1-1.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/2-1.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/3-1.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/4-1.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/5-1.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/6-1.png)\n",
    "\n",
    "Tamanho do episódio por episódio\n",
    "\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/1-4.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/2-4.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/3-4.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/4-4.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/5-4.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/6-4.png)\n",
    "\n",
    "Número de acertos por episódio\n",
    "\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/1-3.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/2-3.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/3-3.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/4-3.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/5-3.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/6-3.png)\n",
    "\n",
    "Taxa de exploração por episódio\n",
    "\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/1-2.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/2-2.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/3-2.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/4-2.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/5-2.png)\n",
    "![Q1](results/SARSA_LAMBDA/ripped_images/6-2.png)\n",
    "\n",
    "Mapa de direções\n",
    "\n",
    "![Q5](results\\SARSA_Lambda\\images\\directions_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8km9jYazzMe"
   },
   "source": [
    "## Problem Implementations With Linear Function Aproximators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcXubztPzzQz"
   },
   "source": [
    "### Monte Carlo Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2ad10451Ffp"
   },
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4IyGuX21Ffq"
   },
   "outputs": [],
   "source": [
    "def simulate(discount_factor:float= 0.99):\n",
    "    \"\"\"Simulate Q-Learning Algorythim .\n",
    "\n",
    "    Args:\n",
    "        discount_factor (float): gamma.\n",
    "\n",
    "    Returns:\n",
    "        np.array(reward_episode_list): Numpy array with the episodes rewards\n",
    "        np.array(explore_rate_episode_list): Numpy array with the episodes explore rates\n",
    "        np.array(num_streaks_episode_list): Numpy array with the episodes number of mazze completions\n",
    "        np.array(length_episode_list): Numpy array with the episodes lengths\n",
    "    \"\"\"\n",
    "\n",
    "    #Defining Feature Vector\n",
    "    X = np.zeros((MAZE_SIZE[0]*MAZE_SIZE[1] , MAZE_SIZE[0]*MAZE_SIZE[1], 4))\n",
    "\n",
    "    X[:,:,0] = np.identity(MAZE_SIZE[0]*MAZE_SIZE[1]) # Vetor de estados para ação 0\n",
    "    X[:,:,1] = np.identity(MAZE_SIZE[0]*MAZE_SIZE[1]) # Vetor de estados para ação 1\n",
    "    X[:,:,2] = np.identity(MAZE_SIZE[0]*MAZE_SIZE[1]) # Vetor de estados para ação 2\n",
    "    X[:,:,3] = np.identity(MAZE_SIZE[0]*MAZE_SIZE[1]) # Vetor de estados para ação 3\n",
    "\n",
    "\n",
    "    #Defining q_hat (oraculo)\n",
    "    q_hat = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    #Defining wights\n",
    "    weights = np.zeros(MAZE_SIZE[0]*MAZE_SIZE[1])\n",
    "\n",
    "    #creating episodes results lists\n",
    "    \n",
    "    reward_episode_list = []\n",
    "    explore_rate_episode_list = []\n",
    "    num_streaks_episode_list = []\n",
    "    length_episode_list = []\n",
    "\n",
    "    # Instantiating the related par ameters\n",
    "    explore_rate = get_explore_rate(0)\n",
    "\n",
    "    # Render tha maze\n",
    "    env.render()\n",
    "\n",
    "    #Initializain num streaks\n",
    "    num_streaks= 0\n",
    "\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        # Episode Reinitialization\n",
    "        total_reward = 0\n",
    "        G_t = 0\n",
    "\n",
    "        # Sample k-th episode (sk,1, ak,1, rk,1, sk,2, . . . , sk,T ) given πk\n",
    "        obv = env.reset()\n",
    "        state = state_to_bucket(obv)\n",
    "        episode_array = []\n",
    "        for t in range(MAX_T):\n",
    "            action = epsilon_greedy_police(state, explore_rate,q_hat)\n",
    "            obv, reward, done, _ = env.step(action)\n",
    "            next_state = state_to_bucket(obv)\n",
    "            total_reward += reward\n",
    "            episode_array.append((state, action, reward)) # tuple transforma array([0,0]) em (0,0)\n",
    "\n",
    "            # Render tha maze\n",
    "            if RENDER_MAZE:\n",
    "                env.render()\n",
    "\n",
    "            if env.is_game_over():\n",
    "                sys.exit()\n",
    "\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {episode} finished after {t} time steps with total reward = {total_reward} (streak {num_streaks}).\")\n",
    "\n",
    "                if t <= SOLVED_T:\n",
    "                    num_streaks += 1\n",
    "                else:\n",
    "                    num_streaks = 0\n",
    "                break\n",
    "\n",
    "            elif t >= MAX_T - 1:\n",
    "                print(f\"Episode {episode} timed out at {t} with total reward = {total_reward}.\")\n",
    "\n",
    "            state = next_state\n",
    "        \n",
    "        # Find all states the we've visited in this episode\n",
    "        # We convert each state to a tuple so that we can use it as a dict key\n",
    "        sa_in_episode = set([(tuple(x[0]), x[1]) for x in episode_array])\n",
    "        for s, a in sa_in_episode:\n",
    "            sa_pair = (s, a)\n",
    "            # Find the first occurance of the (state, action) pair in the episode\n",
    "            first_occurence_idx = next(i for i,x in enumerate(episode_array) if x[0] == s and x[1] == a)\n",
    "\n",
    "            # Sum up all rewards since the first occurance\n",
    "            G_t = sum([x[2]*(discount_factor**i) for i,x in enumerate(episode_array[first_occurence_idx:])])\n",
    "            # Calculate average return for this state over all sampled episodes\n",
    "            weights += (G_t - np.dot( X[:,MAZE_SIZE[0]*s[0]+s[1], a], weights) * X[:,MAZE_SIZE[0]*s[0]+s[1], a])\n",
    "                        \n",
    "            q_hat[s][a] += np.dot( X[:,MAZE_SIZE[0]*s[0]+s[1], a], weights)\n",
    "\n",
    "\n",
    "        # Appending Episode values to the list\n",
    "        reward_episode_list.append(total_reward)\n",
    "        explore_rate_episode_list.append(explore_rate)\n",
    "        num_streaks_episode_list.append(num_streaks)\n",
    "        length_episode_list.append(t)\n",
    "\n",
    "        # It's considered done when it's solved over 120 times consecutively\n",
    "        if num_streaks > STREAK_TO_END:\n",
    "            break\n",
    "\n",
    "        # Update parameters\n",
    "        explore_rate = get_explore_rate(episode)\n",
    "\n",
    "\n",
    "    return (q_hat,np.array(reward_episode_list),\n",
    "            np.array(explore_rate_episode_list),\n",
    "            np.array(num_streaks_episode_list),\n",
    "            np.array(length_episode_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9mU5qjc8UR2"
   },
   "source": [
    "Results (Graphs and tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdL7HLJi8UR4"
   },
   "source": [
    "Recompença pelo episodio \n",
    "\n",
    "![Q1](results\\Monte_Carlo_First_Visit-VFA\\images\\reward_episode.png)\n",
    "\n",
    "Tamanho do episódio por episódio\n",
    "\n",
    "![Q2](results\\Monte_Carlo_First_Visit-VFA\\images\\length_episode.png)\n",
    "\n",
    "Número de acertos por episódio\n",
    "\n",
    "![Q3](results\\Monte_Carlo_First_Visit-VFA\\images\\num_streaks_episode.png)\n",
    "\n",
    "Taxa de exploração por episódio\n",
    "\n",
    "![Q4](results\\Monte_Carlo_First_Visit-VFA\\images\\explore_rate_episode.png)\n",
    "\n",
    "Mapa de direções\n",
    "\n",
    "![Q5](results\\Monte_Carlo_First_Visit-VFA\\images\\directions_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-J3IrIjgzzVr"
   },
   "source": [
    "### Q-learning (or some variation like DoubleQ-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdTY6Doj1GCt"
   },
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2jb9tJT1GCu"
   },
   "outputs": [],
   "source": [
    "def simulate(discount_factor:float= 0.99,\n",
    "             learning_rate:float= 0.01):\n",
    "    \"\"\"Simulate Q-Learning Algorythim .\n",
    "\n",
    "    Args:\n",
    "        discount_factor (float): gamma.\n",
    "\n",
    "    Returns:\n",
    "        np.array(reward_episode_list): Numpy array with the episodes rewards\n",
    "        np.array(explore_rate_episode_list): Numpy array with the episodes explore rates\n",
    "        np.array(num_streaks_episode_list): Numpy array with the episodes number of mazze completions\n",
    "        np.array(length_episode_list): Numpy array with the episodes lengths\n",
    "    \"\"\"\n",
    "\n",
    "    #Defining Feature Vector\n",
    "    X = np.zeros((MAZE_SIZE[0]*MAZE_SIZE[1] , MAZE_SIZE[0]*MAZE_SIZE[1], 4))\n",
    "\n",
    "    X[:,:,0] = np.identity(MAZE_SIZE[0]*MAZE_SIZE[1]) # Vetor de estados para ação 0\n",
    "    X[:,:,1] = np.identity(MAZE_SIZE[0]*MAZE_SIZE[1]) # Vetor de estados para ação 1\n",
    "    X[:,:,2] = np.identity(MAZE_SIZE[0]*MAZE_SIZE[1]) # Vetor de estados para ação 2\n",
    "    X[:,:,3] = np.identity(MAZE_SIZE[0]*MAZE_SIZE[1]) # Vetor de estados para ação 3\n",
    "\n",
    "\n",
    "    #Defining q_hat (oraculo)\n",
    "    q_hat = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    #Defining wights\n",
    "    weights = np.zeros(MAZE_SIZE[0]*MAZE_SIZE[1])\n",
    "\n",
    "    #creating episodes results lists\n",
    "    reward_episode_list = []\n",
    "    explore_rate_episode_list = []\n",
    "    num_streaks_episode_list = []\n",
    "    length_episode_list = []\n",
    "\n",
    "    # Instantiating the learning related parameters\n",
    "    explore_rate = get_explore_rate(0)\n",
    "\n",
    "    # Render tha maze\n",
    "    env.render()\n",
    "    \n",
    "    #Initializain num streaks\n",
    "    num_streaks= 0\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "\n",
    "        # Reset the environment\n",
    "        obv = env.reset()\n",
    "\n",
    "        # the initial state\n",
    "        state = state_to_bucket(obv)\n",
    "        total_reward = 0\n",
    "\n",
    "\n",
    "        for t in range(MAX_T):\n",
    "\n",
    "            # Select an action\n",
    "            action = epsilon_greedy_police(state, explore_rate,q_hat)\n",
    "\n",
    "            # execute the action\n",
    "            obv, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Observe the result\n",
    "            next_state = state_to_bucket(obv)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Update the Q based on the result\n",
    "            weights += learning_rate * ((reward + discount_factor * np.dot( X[:, MAZE_SIZE[0]*next_state[0] + next_state[1], action], weights) - np.dot(X[:,MAZE_SIZE[0]*state[0]+state[1], action], weights)) * X[:,MAZE_SIZE[0]*state[0]+state[1], action])\n",
    "            \n",
    "            q_hat[state][action] += np.dot( X[:, MAZE_SIZE[0]*state[0] + state[1], action], weights)        \n",
    "\n",
    "            # Setting up for the next iteration\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "            # Render tha maze\n",
    "            if RENDER_MAZE:\n",
    "                env.render()\n",
    "\n",
    "            if env.is_game_over():\n",
    "                sys.exit()\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {episode} finished after {t} time steps with total reward = {total_reward} (streak {num_streaks}).\")\n",
    "\n",
    "                if t <= SOLVED_T:\n",
    "                    num_streaks += 1\n",
    "                else:\n",
    "                    num_streaks = 0\n",
    "                break\n",
    "\n",
    "            elif t >= MAX_T - 1:\n",
    "                print(f\"Episode {episode} timed out at {t} with total reward = {total_reward}.\")\n",
    "\n",
    "        # Appending Episode values to the list\n",
    "        reward_episode_list.append(total_reward)\n",
    "        explore_rate_episode_list.append(explore_rate)\n",
    "        num_streaks_episode_list.append(num_streaks)\n",
    "        length_episode_list.append(t)\n",
    "\n",
    "        # It's considered done when it's solved over 120 times consecutively\n",
    "        if num_streaks > STREAK_TO_END:\n",
    "            break\n",
    "\n",
    "        # Update parameters\n",
    "        explore_rate = get_explore_rate(episode)\n",
    "\n",
    "\n",
    "    return (q_hat,np.array(reward_episode_list),\n",
    "            np.array(explore_rate_episode_list),\n",
    "            np.array(num_streaks_episode_list),\n",
    "            np.array(length_episode_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7iyqYFHzzen"
   },
   "source": [
    "#### Results (Graphs and tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Llkt5ley1LZ_"
   },
   "source": [
    "Recompença pelo episodio \n",
    "\n",
    "![Q1](results/Q_LEANING_VFA/ripped_images/1.png)\n",
    "\n",
    "Tamanho do episódio por episódio\n",
    "\n",
    "![Q2](results/Q_LEANING_VFA/ripped_images/4.png)\n",
    "\n",
    "Número de acertos por episódio\n",
    "\n",
    "![Q3](results/Q_LEANING_VFA/ripped_images/3.png)\n",
    "\n",
    "Taxa de exploração por episódio\n",
    "\n",
    "![Q4](results/Q_LEANING_VFA/ripped_images/2.png)\n",
    "\n",
    "Mapa de direções\n",
    "\n",
    "![Q5](results\\Q_Learning-VFA\\images\\directions_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAmJKGGQzzZ6"
   },
   "source": [
    "### SARSA(λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAKTi8kD1Gtk"
   },
   "source": [
    "#### Implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ro3H7izd1Gtk"
   },
   "outputs": [],
   "source": [
    "def simulate(discount_factor:float=0.99,\n",
    "            learning_rate:float=0.01,\n",
    "            epsilon:float= 1,\n",
    "            epsilon_decay:float= 0.999,\n",
    "            trace_decay:int= 0):\n",
    "\n",
    "\n",
    "    \"\"\"Simulate Sarsa Algorythim .\n",
    "\n",
    "    Args:\n",
    "        discount_factor (float): gamma.\n",
    "        epsilon (float): gamma.\n",
    "        epsilon_decay (float): gamma.\n",
    "        trace_decay (float): gamma.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        np.array(reward_episode_list): Numpy array with the episodes rewards\n",
    "        np.array(explore_rate_episode_list): Numpy array with the episodes explore rates\n",
    "        np.array(learning_rate_episode_list): Numpy array with the episodes learning rates\n",
    "        np.array(num_streaks_episode_list): Numpy array with the episodes number of mazze completions\n",
    "        np.array(length_episode_list): Numpy array with the episodes lengths\n",
    "    \"\"\"\n",
    "    \n",
    "    #Defining Feature Vector\n",
    "    X = np.zeros((MAZE_SIZE[0]*MAZE_SIZE[1] , MAZE_SIZE[0]*MAZE_SIZE[1], 4))\n",
    "\n",
    "    X[:,:,0] = np.identity(MAZE_SIZE[0]*MAZE_SIZE[1]) # Vetor de estados para ação 0\n",
    "    X[:,:,1] = np.identity(MAZE_SIZE[0]*MAZE_SIZE[1]) # Vetor de estados para ação 1\n",
    "    X[:,:,2] = np.identity(MAZE_SIZE[0]*MAZE_SIZE[1]) # Vetor de estados para ação 2\n",
    "    X[:,:,3] = np.identity(MAZE_SIZE[0]*MAZE_SIZE[1]) # Vetor de estados para ação 3\n",
    "    \n",
    "    E = X\n",
    "    \n",
    "    weights = np.zeros(MAZE_SIZE[0]*MAZE_SIZE[1])    \n",
    "\n",
    "    \n",
    "    #creating episodes results lists\n",
    "    \n",
    "    reward_episode_list = []\n",
    "    explore_rate_episode_list = []\n",
    "    learning_rate_episode_list = []\n",
    "    num_streaks_episode_list = []\n",
    "    length_episode_list = []\n",
    "\n",
    "\n",
    "    # Instantiating the learning related parameters\n",
    "    explore_rate = get_explore_rate(0)\n",
    "     \n",
    "    # Render tha maze\n",
    "    env.render()\n",
    "    \n",
    "    #q_table # inicializar ela aqui.\n",
    "    q_table = np.zeros(NUM_BUCKETS + (NUM_ACTIONS,), dtype=float)\n",
    "    \n",
    "\n",
    "    #Initializain num streaks\n",
    "    num_streaks= 0\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "\n",
    " \n",
    "        # Reset the environment\n",
    "        obv = env.reset()\n",
    "        \n",
    "        # initialize Reward\n",
    "        total_reward = 0\n",
    "        \n",
    "        # initialize episolon decay\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "        # E(S,A) = 0\n",
    "        # E = np.zeros(NUM_BUCKETS + (NUM_ACTIONS,), dtype=float)\n",
    "        E = X\n",
    "        EOLD = X\n",
    "        \n",
    "        # Initialize state\n",
    "        state = state_to_bucket(obv)\n",
    "        \n",
    "        # initialize action    # VERIFICAR SE SEGUE A POLITICA   \n",
    "        action = epsilon_greedy_police(state, explore_rate, q_table)\n",
    "        \n",
    "        for t in range(MAX_T):\n",
    "\n",
    "            # Select an action\n",
    "            next_action = epsilon_greedy_police(state, explore_rate, q_table)\n",
    "            \n",
    "            # execute the action\n",
    "            obv, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Observe the result\n",
    "            next_state = state_to_bucket(obv)\n",
    "            total_reward += reward\n",
    "            \n",
    "\n",
    "            \n",
    "            # TD - r + gamma * Q(S', A') - Q(S,A)\n",
    "            TD = reward + discount_factor * np.dot( X[:, MAZE_SIZE[0]*next_state[0] + next_state[1], next_action], weights) -  np.dot( X[:,MAZE_SIZE[0]*state[0]+state[1], action], weights) \n",
    "            \n",
    "            # E(S,A) - E(S,A) + 1\n",
    "            E[state + (action,)] = trace_decay * discount_factor * TD *EOLD[state + (action,)] +  X[MAZE_SIZE[0]*state[0]+state[1] , MAZE_SIZE[0]*state[0]+state[1], action]\n",
    "            \n",
    "                        \n",
    "            #For all s E S, a E A(s)\n",
    "            \n",
    "            \n",
    "            # Q(S,A) - Q(S,A) + alpha * lamb * E\n",
    "            weights += learning_rate * TD * E[:,MAZE_SIZE[0]*state[0]+state[1], action]\n",
    "            \n",
    "            q_table[(state) + (action,)] += np.dot( X[:, MAZE_SIZE[0]*state[0] + state[1], action], weights)\n",
    "            \n",
    "            \n",
    "            EOLD = E\n",
    "            # E(S, A) -  gama * trace_decay * E * delta * E(S,A)\n",
    "            # delta = TD\n",
    "            \n",
    "            # E = discount_factor * trace_decay * E\n",
    "            \n",
    "            #S - S'\n",
    "            state = next_state\n",
    "            \n",
    "            # A - A'\n",
    "            action = next_action\n",
    "            \n",
    "\n",
    "            # Render tha maze\n",
    "            if RENDER_MAZE:\n",
    "                env.render()\n",
    "\n",
    "            if env.is_game_over():\n",
    "                sys.exit()\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {episode} finished after {t} time steps with total reward = {total_reward} (streak {num_streaks}).\")\n",
    "\n",
    "                if t <= SOLVED_T:\n",
    "                    num_streaks += 1\n",
    "                else:\n",
    "                    num_streaks = 0\n",
    "                break\n",
    "\n",
    "            elif t >= MAX_T - 1:\n",
    "                print(f\"Episode {episode} timed out at {t} with total reward = {total_reward}.\")\n",
    "\n",
    "        \n",
    "\n",
    "        # Appending Episode values to the list\n",
    "        reward_episode_list.append(total_reward)\n",
    "        explore_rate_episode_list.append(explore_rate)\n",
    "        learning_rate_episode_list.append(learning_rate)\n",
    "        num_streaks_episode_list.append(num_streaks)\n",
    "        length_episode_list.append(t)\n",
    "\n",
    "        # It's considered done when it's solved over 120 times consecutively\n",
    "        if num_streaks > STREAK_TO_END:\n",
    "            break\n",
    "\n",
    "        # Update parameters\n",
    "        explore_rate = get_explore_rate(episode)\n",
    "    \n",
    "\n",
    "    return (q_table,np.array(reward_episode_list),\n",
    "            np.array(explore_rate_episode_list),\n",
    "            np.array(num_streaks_episode_list),\n",
    "            np.array(length_episode_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recompença pelo episodio \n",
    "\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/1-1.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/2-1.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/3-1.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/4-1.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/5-1.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/6-1.png)\n",
    "\n",
    "Tamanho do episódio por episódio\n",
    "\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/1-4.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/2-4.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/3-4.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/4-4.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/5-4.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/6-4.png)\n",
    "\n",
    "Número de acertos por episódio\n",
    "\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/1-3.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/2-3.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/3-3.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/4-3.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/5-3.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/6-3.png)\n",
    "\n",
    "Taxa de exploração por episódio\n",
    "\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/1-2.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/2-2.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/3-2.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/4-2.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/5-2.png)\n",
    "![Q1](results/SARSA_LAMBDA_LFA/ripped_images/6-2.png)\n",
    "\n",
    "Mapa de direções\n",
    "\n",
    "![Q5](results\\SARSA_Lambda-VFA\\images\\directions_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dn6W9NgW8r-s"
   },
   "source": [
    "## Avaliação e discussão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EEQ4TwH8y0P"
   },
   "source": [
    "### Quais as vantagens e desvantagens de usar bootstrapping no seu problema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwIBMI2j9AZ7"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi98ntDw8y-o"
   },
   "source": [
    "### Como a função de recompensa influenciou a qualidade da solução? Seu grupo foi capaz de alcançar a política esperada a partir da função de recompensa definida?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66LIe5wk9A0a"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV7AVbgF8zCS"
   },
   "source": [
    "### Como o aproximador de função influenciou os resultados? Quais foram as vantagens e desvantagens de utilizá-lo em seu problema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wm63V0ZC9BVx"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkB2tHnL9Go2"
   },
   "source": [
    "## Contribuições dos membros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00Sm0wzA9J5x"
   },
   "source": [
    "**Matheus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7uX8fQY9J_5"
   },
   "source": [
    "**Fabiano**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ph5fvJa-9KDm"
   },
   "source": [
    "**Lucas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfDOlKeE9KRc"
   },
   "source": [
    "**Raphael**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ICVvXSNtvuj-",
    "xvS9dywKvuuh",
    "dnnI36SGxY3Y",
    "y_OsCby3wXIS",
    "D8km9jYazzMe",
    "dn6W9NgW8r-s",
    "mkB2tHnL9Go2"
   ],
   "name": "Relatorio.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
