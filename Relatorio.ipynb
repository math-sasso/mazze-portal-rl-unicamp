{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OboI4saPvcU9"
   },
   "source": [
    "# Reinforcement Learning: Project 1\n",
    "\n",
    "* Matheus Gustavo Alves Sasso - 158257\n",
    "* Fabiano\n",
    "* Lucas\n",
    "* Rafael Cortez Sanchez - 094324"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICVvXSNtvuj-"
   },
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a32bJG3vuyU"
   },
   "source": [
    "### Definição do Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldl7WENhxCED"
   },
   "source": [
    "R: Dado um labirinto bidimensional e um agente capaz de navegar por esse labirinto, aprender o menor caminho entre dois pontos fornecidos. O labirinto também possui casas pareadas que transportam o agente de uma casa para a outra, adicionando complexidade ao problema. O código base utiliza a biblioteca do OpenAI Gym e está disponível [neste link](https://github.com/MattChanTK/gym-maze)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5H3sfhOJvu2P"
   },
   "source": [
    "### Formulação MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1o-78v7exFlt"
   },
   "source": [
    "R: O MDP é uma tupla $<S, P, A, R, \\gamma>$ tal que:\n",
    "\n",
    "$S = \\{s^{i, j} | i, j \\in \\mathbb{Z};  0 \\lt i \\leq m;  0 \\lt j \\leq n\\}$, em que $m$ e $n$ são as dimensões do labirinto em número inteiro de casas.\n",
    "\n",
    "$P = \\{P^{r, s} = \\begin{cases}\n",
    "                      1.0               & \\text{se não há paredes entre os estados r e s}\\\\\n",
    "                      0.0               & \\text{caso contrário}\n",
    "                  \\end{cases} \\}$\n",
    "\n",
    "$A = \\{\\text{cima}, \\text{baixo}, \\text{esquerda}, \\text{direita}\\}$\n",
    "\n",
    "$R = \\{R^s = \\begin{cases}\n",
    "                      1.0                        & \\text{caso s seja o estado final}\\\\\n",
    "                      \\frac{-0.1}{m \\times n}    & \\text{caso contrário}\n",
    "             \\end{cases} \\}$\n",
    "\n",
    "$\\gamma = 0.99$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdaNbcmawW66"
   },
   "source": [
    "### O modelo de discretização adotado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoLcp2BXxGLf"
   },
   "source": [
    "R: O problema em si já é discretizado, uma vez que o labirinto é composto por um número inteiro de casas dispostas em um plano. Cada estado é definido por duas coordenadas $i, j \\in \\mathbb{Z}$, representando uma casa desse plano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ikar_GgwW-0"
   },
   "source": [
    "### Como o problema foi modelado \n",
    "**Avaliar de acordo com: custo computacional, optimalidade, influência da função de recompensa, tamanhos dos espaços de estados e de ações**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPIivZvlxGx3"
   },
   "source": [
    "R: Considerando que as dimensões do mapa são $n$ e $m$, temos os seguintes tamanhos para os espaços de estados e ações:\n",
    "\n",
    "$|S| = n \\times m$\n",
    "\n",
    "$|A| = 4$ (As quatro direções cardinais no plano)\n",
    "\n",
    "O custo computacional de resolver a equação de Bellman por sistema linear de equações é, portanto:\n",
    "\n",
    "$O(n^3 m^3)$\n",
    "\n",
    "A solução ótima é uma política $\\pi$ que minimiza o número de ações para chegar à casa final do labirinto a partir da casa inicial. Há uma recompensa negativa quando o agente se move para casas que não são a casa final, e uma recompensa positiva quando o agente atinge a casa final. A recompensa negativa é inversamente proporcional ao tamanho do labirinto, de forma que a penalidade é maior quando se navega em um labirinto pequeno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC0J4haqwXEJ"
   },
   "source": [
    "### Especificidades e restrições da implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4qVItC1xHWL"
   },
   "source": [
    "R: O agente aprende a navegar em um único labirinto: a solução não é generalizada. A interface gráfica da solução foi implementada usando a biblioteca pygame e, caso o programa não seja devidamente finalizado, pode ser necessário reiniciar o interpretador Python antes de reinciar a execução do programa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvS9dywKvuuh"
   },
   "source": [
    "## Ambiente do Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZ4GHoLexYkr"
   },
   "source": [
    "### The nature of your environment (episodic/not episodic, deterministic/stochastic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-KGWxPIxvnx"
   },
   "source": [
    "R:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leYY-YTSxYp-"
   },
   "source": [
    "### What are your terminal states (when they exist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxhVi8q8xvT5"
   },
   "source": [
    "R:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C03h4GXvxYtl"
   },
   "source": [
    "### How is your reward function defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihxvKYozxu-Z"
   },
   "source": [
    "R:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qx8cmju9xYyF"
   },
   "source": [
    "### All parameters employed in your methods (discount factor, step size, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvUQFm3exutp"
   },
   "source": [
    "R:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnnI36SGxY3Y"
   },
   "source": [
    "### Enverionment Class Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrF20a9nxuGK"
   },
   "outputs": [],
   "source": [
    "## Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_OsCby3wXIS"
   },
   "source": [
    "## Problem Implementations Without Linear Function Aproximators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9DfVfhLwXLz"
   },
   "source": [
    "### Monte Carlo Control\n",
    "* Initialize the value function to zero. \n",
    "* Use a time-varying scalar step-size of αt = 1/N(st, at)\n",
    "* Use an  Epsilon−greedy exploration strategy with Epsilon_t = N0/(N0+N(st)), where N0 is a constant, N(s) is the number of times that state s has been visited, and N(s, a) is the number of times that action a has been selected\n",
    "from state s. \n",
    "* You should define N0 to fit your problem. \n",
    "* Plot the optimal value function V(s) = maxaQ∗ (s, a).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "393cixyl0WeY"
   },
   "source": [
    "Algorithim Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsmFc9Yl0e2A"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEZeUiD6zX1a"
   },
   "source": [
    "Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldynD963zD9Z"
   },
   "outputs": [],
   "source": [
    "## Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0cl8m_W8Ytm"
   },
   "source": [
    "Results (Graphs and tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCTlIGTQ8Ytm"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUrHUajQ8Ytn"
   },
   "source": [
    "Relationship between adopted parameters x solution performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azYZRYX38Ytn"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hEQLlB88Ytn"
   },
   "source": [
    "Comparition With the traditional algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADkHp8fz8Ytn"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPdtT1Z3wXTk"
   },
   "source": [
    "### Q-learning (or some variation like DoubleQ-learning)\n",
    "* Initialize the action-value function to zero. \n",
    "* Use a random selection of actions whenever you have a draw among actions.\n",
    "* Use the same step-size and exploration schedules as MC Control. \n",
    "* Define the number of episodes and discount factor accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5bWxF9G0lGj"
   },
   "source": [
    "Algorithim Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeOvkG270nvd"
   },
   "source": [
    "R:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJpph5dRz2qC"
   },
   "source": [
    "Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQFtKqKEz4MP"
   },
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOX7sdwc8XVu"
   },
   "source": [
    "Results (Graphs and tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pZOjgIP8XVu"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vP7BqkH78XVu"
   },
   "source": [
    "Relationship between adopted parameters x solution performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNyzudIA8XVv"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSIaaf4S8XVv"
   },
   "source": [
    "Comparition With the traditional algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnVWP1yF8XVv"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJ_UJdr3wXPa"
   },
   "source": [
    "### SARSA(λ): \n",
    "* Initialize the action-value function to zero. \n",
    "* Use a random selection of actions whenever you have a draw among actions. \n",
    "* Use the same step-size and exploration schedules as MC Control.\n",
    "* Use the same number of episodes as Q-learning. \n",
    "* Vary λ ∈ {0, 0.2, ..., 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcJ3p_KF0q4n"
   },
   "source": [
    "Algorithim Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eAKK1R10rEY"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ9O9imAzzIU"
   },
   "source": [
    "Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06H3Msr-0S0m"
   },
   "outputs": [],
   "source": [
    "## Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3jD__Qz8WCH"
   },
   "source": [
    "Results (Graphs and tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKE1p_118WCH"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVQAuYc38WCH"
   },
   "source": [
    "Relationship between adopted parameters x solution performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tjvp4A818WCI"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vc-C6PZG8WCI"
   },
   "source": [
    "Comparition With the traditional algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1A2uhTE8WCI"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8km9jYazzMe"
   },
   "source": [
    "## Problem Implementations With Linear Function Aproximators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcXubztPzzQz"
   },
   "source": [
    "### Monte Carlo Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDOuc8Y21Ffd"
   },
   "source": [
    "Algorithim Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0ngPIxV1Ffp"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2ad10451Ffp"
   },
   "source": [
    "Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4IyGuX21Ffq"
   },
   "outputs": [],
   "source": [
    "## Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9mU5qjc8UR2"
   },
   "source": [
    "Results (Graphs and tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdL7HLJi8UR4"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLVY7uWP8UR4"
   },
   "source": [
    "Relationship between adopted parameters x solution performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VL116Noo8UR4"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcNbAO868UR5"
   },
   "source": [
    "Comparition With the traditional algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XiF8CgS8UR5"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-J3IrIjgzzVr"
   },
   "source": [
    "### Q-learning (or some variation like DoubleQ-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2II_Ui8G1GCs"
   },
   "source": [
    "Algorithim Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-Y42JiT1GCt"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdTY6Doj1GCt"
   },
   "source": [
    "Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2jb9tJT1GCu"
   },
   "outputs": [],
   "source": [
    "## Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7iyqYFHzzen"
   },
   "source": [
    "Results (Graphs and tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Llkt5ley1LZ_"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_lYLjde77_A"
   },
   "source": [
    "Relationship between adopted parameters x solution performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NpHlXDV78LC"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_X6vVpnW8GHQ"
   },
   "source": [
    "Comparition With the traditional algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Msmj-HL8NVK"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAmJKGGQzzZ6"
   },
   "source": [
    "### SARSA(λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aeD_m6-1Gtj"
   },
   "source": [
    "Algorithim Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1oo6Ef41Gtk"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAKTi8kD1Gtk"
   },
   "source": [
    "Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ro3H7izd1Gtk"
   },
   "outputs": [],
   "source": [
    "## Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dn6W9NgW8r-s"
   },
   "source": [
    "## Evaluation and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EEQ4TwH8y0P"
   },
   "source": [
    "### The advantages and disadvantages of bootstrapping in your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwIBMI2j9AZ7"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi98ntDw8y-o"
   },
   "source": [
    "### How the reward function influenced the quality of the solution. Was your group able to achieve the expected policy given the reward function defined?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66LIe5wk9A0a"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV7AVbgF8zCS"
   },
   "source": [
    "### How function approximation influenced the results. What were the advantages and disadvantages of using it in your problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wm63V0ZC9BVx"
   },
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkB2tHnL9Go2"
   },
   "source": [
    "## Member Cotributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00Sm0wzA9J5x"
   },
   "source": [
    "**Matheus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7uX8fQY9J_5"
   },
   "source": [
    "**Fabiano**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ph5fvJa-9KDm"
   },
   "source": [
    "**Lucas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfDOlKeE9KRc"
   },
   "source": [
    "**Raphael**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ICVvXSNtvuj-",
    "xvS9dywKvuuh",
    "dnnI36SGxY3Y",
    "y_OsCby3wXIS",
    "D8km9jYazzMe",
    "dn6W9NgW8r-s",
    "mkB2tHnL9Go2"
   ],
   "name": "Relatorio.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
